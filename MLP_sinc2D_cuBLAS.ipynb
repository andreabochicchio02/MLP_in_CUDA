{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGnVvyGCCcA3FYT7rF/6y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "CUDA",
      "language": "CUDA",
      "name": "xcuda"
    },
    "language_info": {
      "codemirror_mode": "text/x-cuda",
      "file_extension": ".cu",
      "mimetype": "text/x-cuda",
      "name": "cuda",
      "version": "CUDA 12.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabochicchio02/MLP_in_CUDA/blob/main/MLP_sinc2D_cuBLAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron (MLP) for REGRESSION**"
      ],
      "metadata": {
        "id": "G2HX5_z0D8Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer Perceptron to solve a regression problem (sinc2D function) using **sgemm** from **cuBLAS** library"
      ],
      "metadata": {
        "id": "P9hL_3UwHA24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU su COLAB:** Runtime > Cambia tipo di runtime > GPU > Salva"
      ],
      "metadata": {
        "id": "wzvNs0PygLvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including libraries and initializing MLP parameters"
      ],
      "metadata": {
        "id": "AUmMmQULM12X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MLP.cuh\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <array>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <random>\n",
        "#include <ctime>\n",
        "#include <limits>\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "const int num_train = 150*150;              // number of training pattern (put a square number here)\n",
        "const int num_test = 2500;\n",
        "\n",
        "\n",
        "// //////////////////////////////////////////// //\n",
        "//                 MLP parameters               //\n",
        "// //////////////////////////////////////////// //\n",
        "const int n_output = 1;                     // Number of outputs\n",
        "const int n_features = 2;                   // Number of input features\n",
        "const int n_hidden = 300;                   // Number of neurons in the hidden layer\n",
        "const int epochs = 500;                     // Number of epochs\n",
        "const int minibatches = 30;                 // Number of mini-batches\n",
        "__device__ float eta = 1e-6;                // Learning rate\n",
        "\n",
        "\n",
        "// WEIGHTS MODEL AND BIAS\n",
        "array<array<float, n_features+1>, n_hidden> w1 = {};\n",
        "array<array<float, n_hidden+1>, n_output> w2 = {};\n",
        "\n",
        "\n",
        "// Global declaration of variable used in the train step\n",
        "const int elem = (num_train + minibatches -1 )/minibatches;     // inputs used in each minibatch\n",
        "\n",
        "// Threads kernel CUDA\n",
        "unsigned int tilex = 32;    // thread-block x\n",
        "unsigned int tiley = 32;    // thread-block y\n",
        "\n",
        "// GPU DATA\n",
        "float* dev_x_train;         // dataset train\n",
        "float* dev_y_train;         // dataset train\n",
        "float* dev_w1;              // pesi w1\n",
        "float* dev_w2;              // pesi  w2\n",
        "int* dev_I;                 // matrice per indice dataset\n",
        "\n",
        "//forward\n",
        "float* dev_x_input;         // minibatch ingresso MLP\n",
        "float* dev_y_input;         // minibatch ingresso MLP\n",
        "float* dev_a0;              // input and bias\n",
        "float* dev_rZ1;             // ris somma pesata\n",
        "float* dev_rA1;             // uscita hidden layer\n",
        "float* dev_a1;              // ingresso output layer\n",
        "float* dev_rZ2;             // ris somma pesata output layer\n",
        "float* dev_rA2;             // uscita MLP\n",
        "\n",
        "float* dev_square_error;\n",
        "\n",
        "//backpropagation\n",
        "float* dev_dL_dZ2;\n",
        "float* dev_dL_dW2;\n",
        "float* dev_dL_dA1;\n",
        "float* dev_sigma_prime;\n",
        "float* dev_dL_drZ1;\n",
        "float* dev_dL_dW1;\n",
        "\n",
        "float* dev_delta_W1_unscaled;\n",
        "float* dev_delta_W2_unscaled;\n",
        "\n",
        "// predict\n",
        "float* dev_x_pred;            // dataset train in predict (inizialization in predict)\n",
        "float* dev_y_pred;            // dataset train in predict (inizialization in predict)\n",
        "\n",
        "// handle cuBLAS\n",
        "cublasHandle_t handle;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XjoHUwcLP9S",
        "outputId": "3af79aa1-9f5a-4cc3-fffc-c0c6b80d9ef8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MLP.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU allocation"
      ],
      "metadata": {
        "id": "rlHGvbdnhSiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile allocation.cu\n",
        "\n",
        "void GPU_alloc(){\n",
        "    cudaMalloc((void **)&dev_x_train, num_train * n_features * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_w1, n_hidden * (n_features+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_w2, n_output * (n_hidden+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_y_train, num_train * n_output * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_I, minibatches * elem * sizeof(int));\n",
        "    cudaMalloc((void **)&dev_x_input, elem * n_features * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_y_input, elem * n_output * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_a0, (n_features+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rZ1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rA1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_a1, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rZ2, n_output * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rA2, n_output * elem * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_square_error, elem * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_dL_dZ2, n_output * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dW2, n_output * (n_hidden+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dA1, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_sigma_prime, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_drZ1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dW1, n_hidden * (n_features+1) * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_delta_W1_unscaled, n_hidden * (n_features+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_delta_W2_unscaled, n_output * (n_hidden+1) * sizeof(float));\n",
        "\n",
        "    cublasCreate(&handle);\n",
        "}\n",
        "\n",
        "void GPU_free(){\n",
        "    cudaFree(dev_x_train);\n",
        "    cudaFree(dev_w1);\n",
        "    cudaFree(dev_w2);\n",
        "    cudaFree(dev_y_train);\n",
        "\n",
        "    cudaFree(dev_I);\n",
        "    cudaFree(dev_x_input);\n",
        "    cudaFree(dev_y_input);\n",
        "    cudaFree(dev_a0);\n",
        "    cudaFree(dev_rZ1);\n",
        "    cudaFree(dev_rA1);\n",
        "    cudaFree(dev_a1);\n",
        "    cudaFree(dev_rZ2);\n",
        "    cudaFree(dev_rA2);\n",
        "\n",
        "    cudaFree(dev_square_error);\n",
        "\n",
        "    cudaFree(dev_dL_dZ2);\n",
        "    cudaFree(dev_dL_dW2);\n",
        "    cudaFree(dev_dL_dA1);\n",
        "    cudaFree(dev_sigma_prime);\n",
        "    cudaFree(dev_dL_drZ1);\n",
        "    cudaFree(dev_dL_dW1);\n",
        "\n",
        "    cudaFree(dev_delta_W1_unscaled);\n",
        "    cudaFree(dev_delta_W2_unscaled);\n",
        "\n",
        "    cublasDestroy(handle);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vgW5L5phegb",
        "outputId": "c8f62035-8507-4a92-87a2-d03c1c9e1d40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing allocation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA Kernel"
      ],
      "metadata": {
        "id": "9IcFUYmahx99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel.cu\n",
        "\n",
        "__global__ void setMinibatchIdx(int* dev_I, int minibatches, int elem) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\n",
        "    if (idx_x < elem && idx_y < minibatches){\n",
        "        int i = idx_y * elem + idx_x;\n",
        "\n",
        "        int row = i % minibatches;\n",
        "        int col = i / minibatches;\n",
        "\n",
        "        dev_I[row * elem + col] = i;                // set index\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void setBatch(float* x, float* x_input, int row, int col, int m) {\n",
        "    int tid_x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int tid_y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "    if (tid_x < col && tid_y < row) {\n",
        "        int idx = tid_y + (m - 1) * row;\n",
        "        int x_idx = idx * col + tid_x;\n",
        "        int x_input_idx = tid_y * col + tid_x;\n",
        "\n",
        "        x_input[x_input_idx] = x[x_idx];      // Batch copy of data for MLP input\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Transpose matrix and set first row to 1\n",
        "__global__ void transpose_extend(float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x <= col) {\n",
        "        output[idx_x * row + idx_y] = (idx_x == 0) ? 1.0f : input[idx_y * col + (idx_x-1)];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void func_sigmoid(float* output, const float* input, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = 1.0f / (1.0f + expf(-input[idx]));\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sigmoid_gradient(float* output, const float* rA1, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = rA1[idx] * (1.0 - rA1[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void extend_1(float* output, float* input, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x <= col) {\n",
        "        output[idx_y * col + idx_x] = (idx_y == 0) ? 1.0f : input[(idx_y - 1) * col + idx_x];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void elem_mult_elem(float* A, const float* B, float* C, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        C[idx] = A[idx] * B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void elem_sub(float* A, const float* B, float* C, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        C[idx] = A[idx] - B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void copy(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = input[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void update_weights(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] -= eta * input[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void square_error(float* A, const float* B, float* C, int n_hidden, int elem) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    int idx = idx_y * elem + idx_x;\n",
        "\n",
        "    if (idx_y < n_hidden && idx_x < elem) {\n",
        "        float diff = A[idx] - B[idx];\n",
        "        C[idx] = diff * diff;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void initialize_weights(float *W, curandState *states, long seed, int rows, int cols) {\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int idx = idx_y * cols + idx_x;\n",
        "\n",
        "    if (idx_y < rows && idx_x < cols) {\n",
        "        curand_init(seed+idx, 0, 0, &states[idx]);\n",
        "\n",
        "        curandState localState = states[idx];\n",
        "        W[idx] = 2.0f * curand_uniform(&localState) - 1.0f;\n",
        "        states[idx] = localState;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void reduce(float *y, float *x, int N){\n",
        "    extern __shared__ float tsum[];\n",
        "\n",
        "    int id = threadIdx.x;\n",
        "    int tid = blockDim.x*blockIdx.x+threadIdx.x;\n",
        "    int stride =  gridDim.x*blockDim.x;\n",
        "\n",
        "    tsum[id] = 0.0f;\n",
        "    for(int k=tid;k<N;k+=stride)\n",
        "        tsum[id] += x[k];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // number of blocks power of 2 greater than blockDim\n",
        "    int block2 = 1;\n",
        "    while (block2 < blockDim.x) block2 *= 2;\n",
        "\n",
        "    // power of 2 reduction loop\n",
        "    for(int k=block2/2; k>0; k >>= 1){\n",
        "        if(id<k && id+k < blockDim.x)\n",
        "            tsum[id] += tsum[id+k];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if(id==0) y[blockIdx.x] = tsum[0];\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ohbFzCh1TP",
        "outputId": "d4fdb4ee-7732-45df-b2f4-8aee3c983652"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Math function using cuBLAS library"
      ],
      "metadata": {
        "id": "Vmw6bxs_NNx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile math_function.cu\n",
        "\n",
        "void A_mult_B(const float* A, const float* B, float* C,\n",
        "              int rigA, int colA, int colB) {\n",
        "\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C^T = A^T * B^T ==> C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_N, CUBLAS_OP_N,   // C = B * A\n",
        "            colB, rigA, colA,\n",
        "            &alpha,\n",
        "            B, colB,                // Device stride (col-major)\n",
        "            A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            C, colB);               // Device stride (col-major)\n",
        "}\n",
        "\n",
        "\n",
        "void A_mult_B_T(const float* A, const float* B, float* C,\n",
        "                   int rigA, int colA, int rigB) {\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C_T = A_T * B_T => C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_T, CUBLAS_OP_N,   // C = B^T * A\n",
        "            rigB, rigA, colA,\n",
        "            &alpha,\n",
        "            B, colA,                // Device stride (col-major)\n",
        "            A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            C, rigB);               // Device stride (col-major)\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "void A_T_mult_B(const float* A, const float* B, float* C,\n",
        "                int rigA, int colA, int colB) {\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C^T = A^T * B^T ==> C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_N, CUBLAS_OP_T,   // C = B * A^T\n",
        "            colB, colA, rigA,\n",
        "            &alpha,\n",
        "            B, colB,                // Device stride (col-major)\n",
        "            A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            C, colB);               // Device stride (col-major)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB7y475aNnXC",
        "outputId": "f8fd8261-1de2-4543-ea1f-448e4bbe4bba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing math_function.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "Initialize train and test input and output"
      ],
      "metadata": {
        "id": "rYoZ3j5TQsG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gen.cu\n",
        "\n",
        "//sinc2D function generation\n",
        "void sinc2D_gen(float* x, float* y, int num_patterns){\n",
        "    int num_points = sqrt(num_patterns);\n",
        "\n",
        "    // linspace x1\n",
        "    vector<float> x1(num_points);\n",
        "    float start_x1 = -5.0;\n",
        "    float end_x1 = 5.0;\n",
        "    float step_x1 = (end_x1 - start_x1) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x1[i] = start_x1 + i * step_x1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // linspace x2\n",
        "    vector<float> x2(num_points);\n",
        "    float start_x2 = -5.0;\n",
        "    float end_x2 = 5.0;\n",
        "    float step_x2 = (end_x2 - start_x2) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x2[i] = start_x2 + i * step_x2;\n",
        "    }\n",
        "\n",
        "\n",
        "    // meshgrid\n",
        "    vector<vector<float>> XX1(num_points, vector<float>(num_points));\n",
        "    vector<vector<float>> XX2(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        for (int j = 0; j < num_points; ++j){\n",
        "            XX1[i][j] = x1[j];\n",
        "            XX2[i][j] = x2[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // sinc2D\n",
        "    vector<vector<float>> YY(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            float sinc_x1 = (XX1[i][j] == 0) ? 1.0 : sin(XX1[i][j]) / XX1[i][j];\n",
        "            float sinc_x2 = (XX2[i][j] == 0) ? 1.0 : sin(XX2[i][j]) / XX2[i][j];\n",
        "            YY[i][j] = 10.0 * sinc_x1 * sinc_x2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // initialization x e y\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            x[(i*num_points+j)*n_features] = XX1[j][i];\n",
        "            x[(i*num_points+j)*n_features + 1] = XX2[j][i];\n",
        "            y[i * num_points + j] = YY[j][i];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEaYlNe2RMwv",
        "outputId": "8d2b350f-6ca8-46ee-c97b-4fcf8c2deb86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gen.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward function\n",
        "Compute the forward step"
      ],
      "metadata": {
        "id": "EFj34uQaN8AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forward.cu\n",
        "#include \"kernel.cu\"\n",
        "\n",
        "// Compute the forward step\n",
        "void MLP_MSELIN_forward(int elem){\n",
        "    // rA0: is the \"reduced A0\" and it coincides with the transpose of the tall input matrix (nObs x nInput)\n",
        "    // A0  = E(rA0) It is the \"extended\" version of rA0, obtained by it by adding a row of ones as its new first row\n",
        "    // Extend matrix X by adding the bias\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(n_features+threads.x-1)/threads.x, (elem+threads.y-1)/threads.y, 1};\n",
        "    transpose_extend<<<blocks, threads>>>(dev_x_input, dev_a0, elem, n_features);\n",
        "\n",
        "\n",
        "    // rZ1 = \\sum(W1,A0).  It is the pre-activation at layer 1 (the hidden one)\n",
        "    A_mult_B(dev_w1, dev_a0, dev_rZ1, n_hidden, n_features+1, elem);\n",
        "\n",
        "\n",
        "    // rA1 = \\sigma(rZ1).  It is the output of the first layer (the hidden one)\n",
        "    blocks ={(elem+threads.x-1)/threads.x, (n_hidden+threads.y-1)/threads.y, 1};\n",
        "    func_sigmoid<<<blocks, threads>>>(dev_rA1, dev_rZ1, n_hidden, elem);\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A1  = E(rA1).       It is the extended version of rA1\n",
        "    // Extend matrix X by adding the bias\n",
        "    blocks ={(elem+threads.x-1)/threads.x, ((n_hidden+1)+threads.y-1)/threads.y, 1};\n",
        "    extend_1<<<blocks, threads>>>(dev_a1, dev_rA1, (n_hidden+1), elem);\n",
        "\n",
        "\n",
        "    // rZ2 = \\sum(W2,A1).  It is the pre-activation at layer 2 (the output one)\n",
        "    A_mult_B(dev_w2, dev_a1, dev_rZ2, n_output, n_hidden+1, elem);\n",
        "\n",
        "    dev_rA2 = dev_rZ2;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVAQA38KOUa_",
        "outputId": "a2a339e7-b4a5-4a97-f1bd-b1e1a53bfa7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forward.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "Learn weights from training data"
      ],
      "metadata": {
        "id": "-j9j0MX0PMIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.cu\n",
        "#include \"math_function.cu\"\n",
        "#include \"forward.cu\"\n",
        "#include \"allocation.cu\"\n",
        "\n",
        "float MLP_MSE_cost(int elem) {\n",
        "    // Calculate the differences between y and rA2\n",
        "    dim3 threads = {tilex, 1, 1};\n",
        "    dim3 blocks ={(elem + threads.x-1)/threads.x,1,1};\n",
        "    square_error<<<blocks, threads>>>(dev_rA2, dev_y_input, dev_square_error, n_output, elem);\n",
        "\n",
        "\n",
        "    float* dev_temp_sum;        //partial sum of block\n",
        "    cudaMalloc((void **)&dev_temp_sum, blocks.x * sizeof(float));\n",
        "\n",
        "    //partial sum for each block\n",
        "    reduce<<<blocks,threads, threads.x*sizeof(float)>>>(dev_temp_sum, dev_square_error, elem);      //somma parziale ogni blocco\n",
        "\n",
        "    int blocchi = blocks.x;\n",
        "    reduce<<<1, blocks.x, blocks.x * sizeof(float)>>>(dev_square_error, dev_temp_sum, blocchi);\n",
        "\n",
        "    float res;\n",
        "    cudaMemcpy(&res, &dev_square_error[0], sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_temp_sum);\n",
        "\n",
        "    return res;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute the partial derivative of the loss with respect to the two weighting matrices W2 and W1, using the backpropagation algorithm.\n",
        "void MLP_MSELIN_backprop(){\n",
        "    // rA2 is 1xB\n",
        "    // A1  is (H+1)xB\n",
        "    // A0  is (D+1)xB\n",
        "    // rZ1 is HxB\n",
        "    // y   is 1xB\n",
        "    // W1  is Hx(D+1)\n",
        "    // W2  is 1x(H+1)\n",
        "\n",
        "    // Step 1: compute dL_dZ2 of size 1xB\n",
        "    // NB: rA2 coincides with y_pred\n",
        "    // NB: dL_dZe could be called \"grad2\", the gradient on the output layer with respect the pre-activation Z2\n",
        "\n",
        "    dim3 threads = {tilex, 1, 1};\n",
        "    dim3 blocks ={(elem + threads.x-1)/threads.x,1,1};\n",
        "    elem_sub<<<blocks, threads>>>(dev_rA2, dev_y_input, dev_dL_dZ2, n_output, elem);\n",
        "\n",
        "\n",
        "    // Step 2: compute dL_dW2 % size 1x(H+1)\n",
        "    // NB: dL_dW2 could be called \"delta_W2_unscaled\", because it is of the same size of W2 and stores the unscaled variation\n",
        "    A_mult_B_T(dev_dL_dZ2, dev_a1, dev_dL_dW2, n_output, elem, n_hidden+1);\n",
        "\n",
        "    // Step 3: compute dL_dA1 of size (H+1)xB\n",
        "    A_T_mult_B(dev_w2, dev_dL_dZ2, dev_dL_dA1, n_output, n_hidden+1, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 4: compute dL_drZ1 of size HxB (also sigma_prime_of_rZ1 has size HxB)\n",
        "    // NB: dL_drZ1 could have been called \"grad1\", since it is the gradient at the first layer (the hidden one), with respect to Z1\n",
        "    threads = {tilex, tiley, 1};\n",
        "    blocks ={(elem + threads.x-1)/threads.x,(n_hidden+threads.y-1)/threads.y,1};\n",
        "    sigmoid_gradient<<<blocks, threads>>>(dev_sigma_prime, dev_rA1, n_hidden, elem);\n",
        "    elem_mult_elem<<<blocks, threads>>>(&dev_dL_dA1[elem], dev_sigma_prime, dev_dL_drZ1, n_hidden, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 5: compute dL_dW1 of size Hx(D+1)\n",
        "    // NB: dL_dW1 could be called \"delta_W1_unscaled\", because it is of the same size of W2 and stores the unscaled variation of W1\n",
        "    A_mult_B_T(dev_dL_drZ1, dev_a0, dev_dL_dW1, n_hidden, elem, n_features+1);\n",
        "\n",
        "\n",
        "    blocks ={((n_features+1) + threads.x-1)/threads.x,(n_hidden+threads.y-1)/threads.y,1};\n",
        "    copy<<<blocks, threads>>>(dev_dL_dW1, dev_delta_W1_unscaled, n_hidden, (n_features+1));\n",
        "\n",
        "\n",
        "    blocks ={((n_hidden+1) + threads.x-1)/threads.x,(n_output+threads.y-1)/threads.y,1};\n",
        "    copy<<<blocks, threads>>>(dev_dL_dW2, dev_delta_W2_unscaled, n_output, (n_hidden+1));\n",
        "\n",
        "\n",
        "    /* -----------------------------------------------------------------------------\n",
        "    NB: grad2 is the gradient at the hidden layer.\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "\n",
        "    NB: grad1 is the gradient at the hidden layer (derivative\n",
        "    of the loss with respect Z1, the pre-activation at the hidden layer).\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "    ----------------------------------------------------------------------------- */\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// learn weights from training data\n",
        "void MLP_MSELIN_train(const array<array<float, n_features>, num_train> &x, const array<float, num_train> &y){\n",
        "    GPU_alloc();\n",
        "\n",
        "    // initialize weights w1 and w2\n",
        "    random_device rd;\n",
        "    long seed = rd();\n",
        "\n",
        "    curandState *states_w1;         //required for cuRAND\n",
        "    cudaMalloc(&states_w1, n_hidden * (n_features+1) * sizeof(curandState));\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={((n_features+1) + threads.x-1)/threads.x, (n_hidden+threads.y-1)/threads.y,1};\n",
        "    initialize_weights<<<blocks, threads>>>(dev_w1, states_w1, seed, n_hidden, (n_features+1));\n",
        "\n",
        "    curandState *states_w2;         //required for cuRAND\n",
        "    cudaMalloc(&states_w2, n_output * (n_hidden+1) * sizeof(curandState));\n",
        "\n",
        "    blocks ={((n_hidden+1) + threads.x-1)/threads.x, (n_output+threads.y-1)/threads.y,1};\n",
        "    initialize_weights<<<blocks, threads>>>(dev_w2, states_w2, seed, n_output, (n_hidden+1));\n",
        "\n",
        "    cudaFree(states_w1);\n",
        "    cudaFree(states_w2);\n",
        "\n",
        "\n",
        "\n",
        "    // COPIA DATASET GPU\n",
        "    cudaMemcpy(dev_x_train, &x[0][0], num_train * n_features * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_y_train, &y[0], num_train * n_output * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    blocks ={(elem + threads.x-1)/threads.x,(minibatches+threads.y-1)/threads.y,1};\n",
        "    setMinibatchIdx<<<blocks, threads>>>(dev_I, minibatches, elem);\n",
        "\n",
        "    // loop: epochs\n",
        "    for(int e=1; e<=epochs; ++e) {\n",
        "\n",
        "        // loop: minibatches\n",
        "        for(int m=1; m<=minibatches; ++m){\n",
        "\n",
        "            // input for this minibatch\n",
        "            blocks ={(n_features + threads.x-1)/threads.x,(elem+threads.y-1)/threads.y,1};\n",
        "            setBatch<<<blocks, threads>>>(dev_x_train, dev_x_input, elem, n_features, m);\n",
        "\n",
        "            // Feedforward\n",
        "            MLP_MSELIN_forward(elem);\n",
        "\n",
        "\n",
        "            // output for this minibatch\n",
        "            threads ={1,tiley,1};\n",
        "            blocks ={(n_output + threads.x-1)/threads.x,(elem+threads.y-1)/threads.y,1};\n",
        "            setBatch<<<blocks, threads>>>(dev_y_train, dev_y_input, elem, n_output, m);\n",
        "\n",
        "            float res = MLP_MSE_cost(elem);\n",
        "            float step_cost = res / (2.0f * elem);\n",
        "            printf(\"Epoch %d/%d, minibatch %04d, Loss (MSE) %g\\n\", e, epochs, m, step_cost);\n",
        "\n",
        "            // Compute gradient via backpropagation\n",
        "            MLP_MSELIN_backprop();\n",
        "\n",
        "            threads ={tilex,tiley,1};\n",
        "            blocks ={((n_features+1) + threads.x-1)/threads.x,(n_hidden + threads.y-1)/threads.y,1};\n",
        "            update_weights<<<blocks, threads>>>(dev_delta_W1_unscaled, dev_w1, n_hidden , (n_features+1));\n",
        "\n",
        "            blocks ={((n_hidden+1) + threads.x-1)/threads.x,(n_output+ threads.y-1)/threads.y,1};\n",
        "            update_weights<<<blocks, threads>>>(dev_delta_W2_unscaled, dev_w2, n_output, (n_hidden+1));\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(&w1[0][0], dev_w1, n_hidden * (n_features+1) * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&w2[0][0], dev_w2, n_output * (n_hidden+1) * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAp5i-y-Phr6",
        "outputId": "996daf36-7dc0-48ab-e314-e93bcc355418"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ],
      "metadata": {
        "id": "_-qAi8ZCYnCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.cu\n",
        "\n",
        "// Predict the outputs for all the observations in X, where each row of X is a distinct observation.\n",
        "float MLP_MSELIN_predict(float* x, float* y, int tot_elem) {\n",
        "    float sum = 0;\n",
        "\n",
        "    cudaMalloc((void **)&dev_x_pred, tot_elem * n_features * sizeof(float));\n",
        "    cudaMemcpy(dev_x_pred, x, tot_elem * n_features * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMalloc((void **)&dev_y_pred,  tot_elem * n_output * sizeof(float));\n",
        "    cudaMemcpy(dev_y_pred, y, tot_elem * n_output * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    for (int m = 1; m * elem <= tot_elem+(elem-1); ++m) {   // precautions in case tot_elem is not a multiple of elem\n",
        "        int current_elem = (m * elem <= tot_elem) ? elem : (tot_elem - (m - 1) * elem);\n",
        "\n",
        "        dim3 threads ={tilex,tiley,1};\n",
        "        dim3 blocks ={(n_features + threads.x-1)/threads.x,(current_elem+threads.y-1)/threads.y,1};\n",
        "        setBatch<<<blocks, threads>>>(dev_x_pred, dev_x_input, current_elem, n_features, m);\n",
        "\n",
        "        blocks ={(n_output + threads.x-1)/threads.x,(current_elem+threads.y-1)/threads.y,1};\n",
        "        setBatch<<<blocks, threads>>>(dev_y_pred, dev_y_input, current_elem, n_output, m);\n",
        "\n",
        "        MLP_MSELIN_forward(current_elem);\n",
        "\n",
        "        sum += MLP_MSE_cost(current_elem);\n",
        "    }\n",
        "\n",
        "    float step_cost = sum / (2.0f * tot_elem);\n",
        "\n",
        "    cudaFree(dev_x_pred);\n",
        "    cudaFree(dev_y_pred);\n",
        "\n",
        "    return step_cost;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrxrm42Xpohp",
        "outputId": "21ef5e8a-98d2-4653-ca4f-96c612ab72ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main"
      ],
      "metadata": {
        "id": "td8vtoltM-Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"MLP.cuh\"\n",
        "#include \"gen.cu\"\n",
        "#include \"train.cu\"\n",
        "#include \"predict.cu\"\n",
        "\n",
        "int main() {\n",
        "    array<array<float, n_features>, num_train> x_train;\n",
        "    array<float, num_train> y_train;\n",
        "    sinc2D_gen(x_train[0].data(), y_train.data(), num_train);\n",
        "\n",
        "    array<array<float, n_features>, num_test> x_test;\n",
        "    array<float, num_test> y_test;\n",
        "    sinc2D_gen(x_test[0].data(), y_test.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Shuffling training data\n",
        "    array<int, num_train> shuffled_ind;\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        shuffled_ind[i] = i;\n",
        "    }\n",
        "\n",
        "    default_random_engine generator(std::time(nullptr));\n",
        "    shuffle(shuffled_ind.begin(), shuffled_ind.end(), generator);\n",
        "\n",
        "    array<array<float, n_features>, num_train> x_train_temp;\n",
        "    array<float, num_train> y_train_temp;\n",
        "\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        x_train_temp[i] = x_train[shuffled_ind[i]];\n",
        "        y_train_temp[i] = y_train[shuffled_ind[i]];\n",
        "    }\n",
        "\n",
        "    x_train = x_train_temp;\n",
        "    y_train = y_train_temp;\n",
        "\n",
        "    // Learn weights from training data\n",
        "    MLP_MSELIN_train(x_train, y_train);\n",
        "\n",
        "\n",
        "    // Predict the outputs for all the observations in X\n",
        "    float acc_train = MLP_MSELIN_predict(x_train[0].data(), y_train.data(), num_train);\n",
        "    printf(\"Training accuracy (MSE): %g\\n\", acc_train);\n",
        "\n",
        "    float acc_test = MLP_MSELIN_predict(x_test[0].data(), y_test.data(), num_test);\n",
        "    printf(\"Test accuracy: (MSE): %g\\n\", acc_test);\n",
        "\n",
        "    GPU_free();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYazpaGfMTs7",
        "outputId": "ba2a2358-35cd-461c-8e6f-a782c29be14e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run"
      ],
      "metadata": {
        "id": "nmhszUoaNDWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm prog\n",
        "!nvcc -lcublas -o prog main.cu"
      ],
      "metadata": {
        "id": "AlJjXm9peTut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./prog"
      ],
      "metadata": {
        "id": "CnZ1ow2seUYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
