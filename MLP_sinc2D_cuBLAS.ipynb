{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9EZSwugYpM76yzf8cYlty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabochicchio02/MLP_in_CUDA/blob/main/MLP_sinc2D_cuBLAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron (MLP) for REGRESSION**"
      ],
      "metadata": {
        "id": "G2HX5_z0D8Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer Perceptron to solve a regression problem (sinc2D function) using **sgemm** from **cuBLAS** library"
      ],
      "metadata": {
        "id": "P9hL_3UwHA24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU su COLAB:** Runtime > Cambia tipo di runtime > GPU > Salva"
      ],
      "metadata": {
        "id": "wzvNs0PygLvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including libraries and initializing MLP parameters"
      ],
      "metadata": {
        "id": "AUmMmQULM12X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MLP.h\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <array>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <random>\n",
        "#include <ctime>\n",
        "#include <limits>\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "\n",
        "const int num_train = 150*150;              // number of training pattern (put a square number here)\n",
        "const int num_test = 2500;\n",
        "\n",
        "\n",
        "// //////////////////////////////////////////// //\n",
        "//                 MLP parameters               //\n",
        "// //////////////////////////////////////////// //\n",
        "const int n_output = 1;                     // Number of outputs\n",
        "const int n_features = 2;                   // Number of input features\n",
        "const int n_hidden = 300;                   // Number of neurons in the hidden layer\n",
        "const int epochs = 500;                     // Number of epochs\n",
        "float eta = 1e-6;                           // Learning rate\n",
        "const int minibatches = 30;                 // Number of mini-batches\n",
        "\n",
        "vector<float> cost;\n",
        "array<array<float, n_features+1>, n_hidden> w1 = {};\n",
        "array<array<float, n_hidden+1>, n_output> w2 = {};\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// Global declaration of variable used in the train step\n",
        "const int elem = (num_train + minibatches -1 )/minibatches;     // inputs used in each minibatch\n",
        "\n",
        "// forward\n",
        "array<array<float, n_features>, elem> x_input;\n",
        "array<array<float, elem>, n_features> rA0;\n",
        "array<array<float, elem>, n_features+1> a0;\n",
        "array<array<float, elem>, n_hidden> rZ1;\n",
        "array<array<float, elem>, n_hidden> rA1;\n",
        "array<array<float, elem>, n_hidden+1> a1;\n",
        "array<array<float, elem>, n_output> rZ2;\n",
        "array<array<float, elem>, n_output> rA2;\n",
        "\n",
        "\n",
        "// backpropagation\n",
        "array<array<float, elem>, n_output> dL_dZ2;\n",
        "array<array<float, n_hidden+1>, n_output> dL_dW2;\n",
        "array<array<float, elem>, n_hidden+1> dL_dA1;\n",
        "array<array<float, elem>, n_hidden> sigma_prime_of_rZ1;\n",
        "array<array<float, elem>, n_hidden> dL_drZ1;\n",
        "array<array<float, n_features+1>, n_hidden> dL_dW1;\n",
        "array<array<float, n_features+1>, n_hidden> delta_W1_unscaled;\n",
        "array<array<float, n_hidden+1>, n_output> delta_W2_unscaled;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XjoHUwcLP9S",
        "outputId": "1e4802eb-b151-4341-9ba9-7d41da3cdc2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MLP.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Math function using cuBLAS library"
      ],
      "metadata": {
        "id": "Vmw6bxs_NNx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile math_function.cu\n",
        "\n",
        "void A_mult_B(const float* A, const float* B, float* C,\n",
        "              int rigA, int colA, int colB) {\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rigA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, colA * colB * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, rigA * colB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rigA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, colA * colB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    cublasCreate(&handle);\n",
        "\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C^T = A^T * B^T ==> C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_N, CUBLAS_OP_N,   // C = B * A\n",
        "            colB, rigA, colA,\n",
        "            &alpha,\n",
        "            dev_B, colB,                // Device stride (col-major)\n",
        "            dev_A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            dev_C, colB);               // Device stride (col-major)\n",
        "\n",
        "\n",
        "    cudaMemcpy(C, dev_C, rigA * colB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "\n",
        "void A_mult_B_T(const float* A, const float* B, float* C,\n",
        "                   int rigA, int colA, int rigB) {\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rigA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, rigB * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, rigA * rigB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rigA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, rigB * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    cublasCreate(&handle);\n",
        "\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C_T = A_T * B_T => C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_T, CUBLAS_OP_N,   // C = B^T * A\n",
        "            rigB, rigA, colA,\n",
        "            &alpha,\n",
        "            dev_B, colA,                // Device stride (col-major)\n",
        "            dev_A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            dev_C, rigB);               // Device stride (col-major)\n",
        "\n",
        "\n",
        "    cudaMemcpy(C, dev_C, rigA * rigB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "\n",
        "void A_T_mult_B(const float* A, const float* B, float* C,\n",
        "                int rigA, int colA, int colB) {\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rigA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, rigA * colB * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, colA * colB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rigA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, rigA * colB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    cublasCreate(&handle);\n",
        "\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "\n",
        "    // Matrix multiplication: C = alpha * A * B + beta * C\n",
        "    // cublasSgemm uses col-major matrices, so the dimensions and matrices are transposed with respect to the row-major (used in C)\n",
        "    // N.B. C^T = A^T * B^T ==> C = B * A\n",
        "    cublasSgemm(handle,\n",
        "            CUBLAS_OP_N, CUBLAS_OP_T,   // C = B * A^T\n",
        "            colB, colA, rigA,\n",
        "            &alpha,\n",
        "            dev_B, colB,                // Device stride (col-major)\n",
        "            dev_A, colA,                // Device stride (col-major)\n",
        "            &beta,\n",
        "            dev_C, colB);               // Device stride (col-major)\n",
        "\n",
        "\n",
        "    cudaMemcpy(C, dev_C, colA * colB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "\n",
        "void elem_mult_elem(const float* A, const float* B, float* C, int rig, int col) {\n",
        "    for (int i = 0; i < rig; ++i) {\n",
        "        for (int j = 0; j < col; ++j) {\n",
        "            C[i*col+j] = A[i*col+j] * B[i*col+j];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB7y475aNnXC",
        "outputId": "3c569faa-5ba7-4961-a259-d00e6ced2e15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing math_function.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "Initialize train and test input and output"
      ],
      "metadata": {
        "id": "rYoZ3j5TQsG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gen.cu\n",
        "\n",
        "//sinc2D function generation\n",
        "void sinc2D_gen(float* x, float* y, int num_patterns){\n",
        "    int num_points = sqrt(num_patterns);\n",
        "\n",
        "    // linspace x1\n",
        "    vector<float> x1(num_points);\n",
        "    float start_x1 = -5.0;\n",
        "    float end_x1 = 5.0;\n",
        "    float step_x1 = (end_x1 - start_x1) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x1[i] = start_x1 + i * step_x1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // linspace x2\n",
        "    vector<float> x2(num_points);\n",
        "    float start_x2 = -5.0;\n",
        "    float end_x2 = 5.0;\n",
        "    float step_x2 = (end_x2 - start_x2) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x2[i] = start_x2 + i * step_x2;\n",
        "    }\n",
        "\n",
        "\n",
        "    // meshgrid\n",
        "    vector<vector<float>> XX1(num_points, vector<float>(num_points));\n",
        "    vector<vector<float>> XX2(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        for (int j = 0; j < num_points; ++j){\n",
        "            XX1[i][j] = x1[j];\n",
        "            XX2[i][j] = x2[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // sinc2D\n",
        "    vector<vector<float>> YY(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            float sinc_x1 = (XX1[i][j] == 0) ? 1.0 : sin(XX1[i][j]) / XX1[i][j];\n",
        "            float sinc_x2 = (XX2[i][j] == 0) ? 1.0 : sin(XX2[i][j]) / XX2[i][j];\n",
        "            YY[i][j] = 10.0 * sinc_x1 * sinc_x2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // initialization x e y\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            x[(i*num_points+j)*n_features] = XX1[j][i];\n",
        "            x[(i*num_points+j)*n_features + 1] = XX2[j][i];\n",
        "            y[i * num_points + j] = YY[j][i];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEaYlNe2RMwv",
        "outputId": "e25fe3c0-0884-46f9-dde4-ee4b5eb5cf19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gen.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward function\n",
        "Compute the forward step"
      ],
      "metadata": {
        "id": "EFj34uQaN8AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forward.cu\n",
        "\n",
        "// This function computes the sigmoid function for a scalar, a vector or a matrix\n",
        "array<array<float, elem>, n_hidden> MLP_sigmoid(const array<array<float, elem>, n_hidden> &z){\n",
        "    array<array<float, elem>, n_hidden> sig;\n",
        "\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            sig[i][j] = 1.0 / (1.0 + exp(-z[i][j]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return sig;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// Compute the forward step\n",
        "void MLP_MSELIN_forward(){\n",
        "    // rA0: is the \"reduced A0\" and it coincides with the transpose of the tall input matrix (nObs x nInput)\n",
        "    for (int i = 0; i < elem; ++i) {\n",
        "        for (int j = 0; j < n_features; ++j) {\n",
        "            rA0[j][i] = x_input[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A0  = E(rA0) It is the \"extended\" version of rA0, obtained by it by adding a row of ones as its new first row\n",
        "    // Extend matrix X by adding the bias\n",
        "    for (int i = 0; i < n_features+1; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            a0[i][j] = (i == 0) ? 1 : rA0[i-1][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // rZ1 = \\sum(W1,A0).  It is the pre-activation at layer 1 (the hidden one)\n",
        "    A_mult_B(w1[0].data(), a0[0].data(), rZ1[0].data(), n_hidden, n_features+1, elem);\n",
        "\n",
        "\n",
        "    // rA1 = \\sigma(rZ1).  It is the output of the first layer (the hidden one)\n",
        "    rA1 = MLP_sigmoid(rZ1);\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A1  = E(rA1).       It is the extended version of rA1\n",
        "    // Extend matrix X by adding the bias\n",
        "    for (int i = 0; i < n_hidden+1; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            a1[i][j] = (i == 0) ? 1 : rA1[i-1][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // rZ2 = \\sum(W2,A1).  It is the pre-activation at layer 2 (the output one)\n",
        "    A_mult_B(w2[0].data(), a1[0].data(), rZ2[0].data(), n_output, n_hidden+1, elem);\n",
        "\n",
        "\n",
        "    rA2 = rZ2;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVAQA38KOUa_",
        "outputId": "880348c2-cd33-413b-f5d5-effef8342f55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forward.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "Learn weights from training data"
      ],
      "metadata": {
        "id": "-j9j0MX0PMIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.cu\n",
        "#include \"math_function.cu\"\n",
        "#include \"forward.cu\"\n",
        "\n",
        "// initialize weights to Unif[-1,1]\n",
        "void MLP_initialize_weights(){\n",
        "    array<float, n_hidden*(n_features+1)> w1_temp;\n",
        "    for(int i=0; i<n_hidden*(n_features+1); ++i){\n",
        "        w1_temp[i] = 2*(static_cast<float>(rand())/RAND_MAX)-1;\n",
        "    }\n",
        "\n",
        "    // reshape\n",
        "    int index = 0;\n",
        "    for (int j = 0; j < (n_features+1); ++j) {              //col\n",
        "        for (int i = 0; i < n_hidden; ++i) {                //row\n",
        "            w1[i][j] = w1_temp[index++];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    array<float, n_output*(n_hidden+1)> w2_temp;\n",
        "    for(int i=0; i<n_output*(n_hidden+1); ++i){\n",
        "        w2_temp[i] = 2*(static_cast<float>(rand())/RAND_MAX)-1;\n",
        "    }\n",
        "\n",
        "    // reshape\n",
        "    index = 0;\n",
        "    for (int j = 0; j < (n_hidden+1); ++j) {\n",
        "        for (int i = 0; i < n_output; ++i) {\n",
        "            w2[i][j] = w2_temp[index++];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute sigmoid gradient\n",
        "array<array<float, elem>, n_hidden> MLP_sigmoid_gradient(const array<array<float, elem>, n_hidden> &Z) {\n",
        "    array<array<float, elem>, n_hidden> A = MLP_sigmoid(Z);\n",
        "\n",
        "    array<array<float, elem>, n_hidden> sigGrad;\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            sigGrad[i][j] = A[i][j] * (1.0 - A[i][j]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return sigGrad;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "float MLP_MSE_cost(const array<float, elem> &y) {\n",
        "    vector<float> diff(y.size());\n",
        "    for (int i = 0; i < y.size(); ++i) {\n",
        "        diff[i] = (y[i] - rA2[0][i]);\n",
        "        diff[i] *= diff[i];\n",
        "    }\n",
        "\n",
        "\n",
        "    float cost = (accumulate(diff.begin(), diff.end(), 0.0))/ (2.0 * y.size());\n",
        "    return cost;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute the partial derivative of the loss with respect to the two weighting matrices W2 and W1, using the backpropagation algorithm.\n",
        "void MLP_MSELIN_backprop(const array<float, elem> &y){ /*float l1, float l2)*/\n",
        "    // rA2 is 1xB\n",
        "    // A1  is (H+1)xB\n",
        "    // A0  is (D+1)xB\n",
        "    // rZ1 is HxB\n",
        "    // y   is 1xB\n",
        "    // W1  is Hx(D+1)\n",
        "    // W2  is 1x(H+1)\n",
        "\n",
        "    // Step 1: compute dL_dZ2 of size 1xB\n",
        "    // NB: rA2 coincides with y_pred\n",
        "    // NB: dL_dZe could be called \"grad2\", the gradient on the output layer with respect the pre-activation Z2\n",
        "    for(int i = 0; i<n_output; i++){\n",
        "        for(int j = 0; j < elem; ++j) {\n",
        "            dL_dZ2[i][j] = rA2[i][j] - y[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // Step 2: compute dL_dW2 % size 1x(H+1)\n",
        "    // NB: dL_dW2 could be called \"delta_W2_unscaled\", because it is of the same size of W2 and stores the unscaled variation\n",
        "    A_mult_B_T(dL_dZ2[0].data(), a1[0].data(), dL_dW2[0].data(), n_output, elem, n_hidden+1);\n",
        "\n",
        "\n",
        "    // Step 3: compute dL_dA1 of size (H+1)xB\n",
        "    A_T_mult_B(w2[0].data(), dL_dZ2[0].data(), dL_dA1[0].data(), n_output, n_hidden+1, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 4: compute dL_drZ1 of size HxB (also sigma_prime_of_rZ1 has size HxB)\n",
        "    // NB: dL_drZ1 could have been called \"grad1\", since it is the gradient at the first layer (the hidden one), with respect to Z1\n",
        "    sigma_prime_of_rZ1 = MLP_sigmoid_gradient(rZ1);\n",
        "    elem_mult_elem(dL_dA1[1].data(), sigma_prime_of_rZ1[0].data(),dL_drZ1[0].data(),  n_hidden, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 5: compute dL_dW1 of size Hx(D+1)\n",
        "    // NB: dL_dW1 could be called \"delta_W1_unscaled\", because it is of the same size of W2 and stores the unscaled variation of W1\n",
        "    A_mult_B_T(dL_drZ1[0].data(), a0[0].data(), dL_dW1[0].data(), n_hidden, elem, n_features+1);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 6: regularise or not\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < n_features+1; ++j) {\n",
        "            delta_W1_unscaled[i][j] = dL_dW1[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int j = 0; j < n_output; ++j) {\n",
        "        for (int i = 0; i < n_hidden+1; ++i) {\n",
        "            delta_W2_unscaled[j][i] = dL_dW2[j][i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    /* -----------------------------------------------------------------------------\n",
        "    NB: grad2 is the gradient at the hidden layer.\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "\n",
        "    NB: grad1 is the gradient at the hidden layer (derivative\n",
        "    of the loss with respect Z1, the pre-activation at the hidden layer).\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "    ----------------------------------------------------------------------------- */\n",
        "}\n",
        "\n",
        "// learn weights from training data\n",
        "void MLP_MSELIN_train(const array<array<float, n_features>, num_train> &x, const array<float, num_train> &y){\n",
        "    // initialize weights w1 and w2\n",
        "    MLP_initialize_weights();\n",
        "\n",
        "    cost.push_back(numeric_limits<float>::infinity());\n",
        "\n",
        "    // loop: epochs\n",
        "    for(int e=1; e<=epochs; e++) {\n",
        "        // Advanced version: adaptive learning rate\n",
        "        // eta = eta / (1+decrease_const*e);\n",
        "\n",
        "\n",
        "        //reshape\n",
        "        array<array<int, elem>, minibatches> I;\n",
        "        for (int i = 0; i < num_train; ++i) {\n",
        "            int row = i % minibatches;\n",
        "            int col = i / minibatches;\n",
        "            I[row][col] = i;\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        // loop: minibatches\n",
        "        for(int m=1; m<=minibatches; ++m){\n",
        "            array<int, elem> idx = I[m-1];\n",
        "\n",
        "            // Compute the forward step\n",
        "            for(int i=0; i<elem; i++) {\n",
        "                copy(x[idx[i]].begin(), x[idx[i]].end(), x_input[i].begin());\n",
        "            }\n",
        "            // Feedforward\n",
        "            MLP_MSELIN_forward();\n",
        "\n",
        "\n",
        "            // Compute cost function\n",
        "            array<float, elem> y_index;\n",
        "            for(int i=0; i<elem; i++) {\n",
        "                y_index[i] = y[idx[i]];\n",
        "            }\n",
        "            float step_cost = MLP_MSE_cost(y_index);\n",
        "            cost.push_back(step_cost);\n",
        "\n",
        "\n",
        "            printf(\"Epoch %d/%d, minibatch %04d, Loss (MSE) %g\\n\", e, epochs, m, step_cost);\n",
        "\n",
        "\n",
        "            // Compute gradient via backpropagation\n",
        "            MLP_MSELIN_backprop(y_index);\n",
        "\n",
        "\n",
        "            array<array<float, n_features+1>, n_hidden> delta_W1;\n",
        "            for (int i = 0; i < n_hidden; ++i) {\n",
        "                for (int j = 0; j < n_features+1; ++j) {\n",
        "                    delta_W1[i][j] = eta * delta_W1_unscaled[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "            array<array<float, n_hidden+1>, n_output> delta_W2;\n",
        "            for (int i = 0; i < n_output; ++i) {\n",
        "                for (int j = 0; j < n_hidden+1; ++j) {\n",
        "                    delta_W2[i][j] = eta * delta_W2_unscaled[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "            for (int i = 0; i < n_hidden; ++i) {\n",
        "                for (int j = 0; j < n_features+1; ++j) {\n",
        "                    w1[i][j] -= delta_W1[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "            for (int i = 0; i < n_output; ++i) {\n",
        "                for (int j = 0; j < n_hidden+1; ++j) {\n",
        "                    w2[i][j] -= delta_W2[i][j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAp5i-y-Phr6",
        "outputId": "ac915b16-acf1-4441-940e-4a91007cb8a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ],
      "metadata": {
        "id": "_-qAi8ZCYnCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.cu\n",
        "\n",
        "/* Predict the outputs for all the observations in X, where each row of X is a distinct observation.*/\n",
        "void MLP_MSELIN_predict(float* x, float* y, int tot_elem) {\n",
        "    for (int i = 0; i < tot_elem; i += elem) {\n",
        "\n",
        "        for (int k = 0; k < elem * n_features; ++k) {\n",
        "            int row = k / n_features;\n",
        "            int col = k % n_features;\n",
        "            x_input[row][col] = x[i * n_features + k];\n",
        "        }\n",
        "\n",
        "        // Feedforward\n",
        "        MLP_MSELIN_forward();\n",
        "\n",
        "        // Copia dei risultati nel vettore y\n",
        "        std::copy(rA2[0].begin(), rA2[0].begin() + elem, y + i);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrxrm42Xpohp",
        "outputId": "101fc2cd-7048-457c-e504-1d16831ebf12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main"
      ],
      "metadata": {
        "id": "td8vtoltM-Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"MLP.h\"\n",
        "#include \"gen.cu\"\n",
        "#include \"train.cu\"\n",
        "#include \"predict.cu\"\n",
        "\n",
        "int main() {\n",
        "    array<array<float, n_features>, num_train> x_train;\n",
        "    array<float, num_train> y_train;\n",
        "    sinc2D_gen(x_train[0].data(), y_train.data(), num_train);\n",
        "\n",
        "    array<array<float, n_features>, num_test> x_test;\n",
        "    array<float, num_test> y_test;\n",
        "    sinc2D_gen(x_test[0].data(), y_test.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Shuffling training data\n",
        "    array<int, num_train> shuffled_ind;\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        shuffled_ind[i] = i;\n",
        "    }\n",
        "\n",
        "    default_random_engine generator(std::time(nullptr));\n",
        "    shuffle(shuffled_ind.begin(), shuffled_ind.end(), generator);\n",
        "\n",
        "    array<array<float, n_features>, num_train> x_train_temp;\n",
        "    array<float, num_train> y_train_temp;\n",
        "\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        x_train_temp[i] = x_train[shuffled_ind[i]];\n",
        "        y_train_temp[i] = y_train[shuffled_ind[i]];\n",
        "    }\n",
        "\n",
        "    x_train = x_train_temp;\n",
        "    y_train = y_train_temp;\n",
        "\n",
        "    // Learn weights from training data\n",
        "    MLP_MSELIN_train(x_train, y_train);\n",
        "\n",
        "\n",
        "\n",
        "   /* Predict the outputs for all the observations in X */\n",
        "    array<float, num_train> ytrain_pred;\n",
        "    MLP_MSELIN_predict(x_train[0].data(), ytrain_pred.data(), num_train);\n",
        "\n",
        "    array<float, num_test> ytest_pred;\n",
        "    MLP_MSELIN_predict(x_test[0].data(), ytest_pred.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Compute accuracy (MSE)\n",
        "    float acc_train = 0.0;\n",
        "    for (int i = 0; i < y_train.size(); ++i) {\n",
        "        acc_train += (y_train[i] - ytrain_pred[i])*(y_train[i] - ytrain_pred[i]);\n",
        "    }\n",
        "    acc_train /= (2 * y_train.size());\n",
        "    printf(\"Training accuracy (MSE): %g\\n\", acc_train);\n",
        "\n",
        "    float acc_test = 0.0;\n",
        "    for (int i = 0; i < y_test.size(); ++i) {\n",
        "        acc_test += (y_test[i] - ytest_pred[i])*(y_test[i] - ytest_pred[i]);\n",
        "    }\n",
        "    acc_test /= (2 * y_test.size());\n",
        "    printf(\"Test accuracy: (MSE): %g\\n\", acc_test);\n",
        "\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYazpaGfMTs7",
        "outputId": "eeb0c908-5592-4a41-8f76-7b7eb8c4a5e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run"
      ],
      "metadata": {
        "id": "nmhszUoaNDWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm prog\n",
        "!nvcc -o prog main.cu -lcublas"
      ],
      "metadata": {
        "id": "AlJjXm9peTut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./prog"
      ],
      "metadata": {
        "id": "CnZ1ow2seUYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}