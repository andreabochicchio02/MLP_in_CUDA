{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPq+wTL0fTRdVPlCOHLcuew",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "CUDA",
      "language": "CUDA",
      "name": "xcuda"
    },
    "language_info": {
        "codemirror_mode": "text/x-cuda",
        "file_extension": ".cu",
        "mimetype": "text/x-cuda",
        "name": "cuda",
        "version": "CUDA 12.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabochicchio02/MLP_in_CUDA/blob/main/MLP_sinc2D_tiled_mult.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron (MLP) for REGRESSION**"
      ],
      "metadata": {
        "id": "3oWxHT5YGIfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer Perceptron to solve a regression problem (sinc2D function) using **GPU shared memory** in CUDA"
      ],
      "metadata": {
        "id": "Qy20u5BVGMEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU su COLAB:** Runtime > Cambia tipo di runtime > GPU > Salva"
      ],
      "metadata": {
        "id": "rR4k56CaGPQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including libraries and initializing MLP parameters"
      ],
      "metadata": {
        "id": "QZKXC_U3GSzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MLP.h\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <array>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <random>\n",
        "#include <ctime>\n",
        "#include <limits>\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <thrust/host_vector.h>\n",
        "#include <thrust/device_vector.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "const int num_train = 150*150;              // number of training pattern (put a square number here)\n",
        "const int num_test = 2500;\n",
        "\n",
        "\n",
        "\n",
        "// //////////////////////////////////////////// //\n",
        "//                 MLP parameters               //\n",
        "// //////////////////////////////////////////// //\n",
        "const int n_output = 1;                     // Number of outputs\n",
        "const int n_features = 2;                   // Number of input features\n",
        "const int n_hidden = 300;                   // Number of neurons in the hidden layer\n",
        "const int epochs = 500;                     // Number of epochs\n",
        "float eta = 1e-6;                           // Learning rate\n",
        "const int minibatches = 30;                 // Number of mini-batches\n",
        "\n",
        "vector<float> cost;\n",
        "array<array<float, n_features+1>, n_hidden> w1 = {};\n",
        "array<array<float, n_hidden+1>, n_output> w2 = {};\n",
        "\n",
        "\n",
        "\n",
        "// Global declaration of variable used in the train step\n",
        "const int elem = (num_train + minibatches -1 )/minibatches;     // inputs used in each minibatch\n",
        "\n",
        "// forward\n",
        "array<array<float, n_features>, elem> x_input;\n",
        "array<array<float, elem>, n_features> rA0;\n",
        "array<array<float, elem>, n_features+1> a0;\n",
        "array<array<float, elem>, n_hidden> rZ1;\n",
        "array<array<float, elem>, n_hidden> rA1;\n",
        "array<array<float, elem>, n_hidden+1> a1;\n",
        "array<array<float, elem>, n_output> rZ2;\n",
        "array<array<float, elem>, n_output> rA2;\n",
        "\n",
        "\n",
        "// backpropagation\n",
        "array<array<float, elem>, n_output> dL_dZ2;\n",
        "array<array<float, n_hidden+1>, n_output> dL_dW2;\n",
        "array<array<float, elem>, n_hidden+1> dL_dA1;\n",
        "array<array<float, elem>, n_hidden> sigma_prime_of_rZ1;\n",
        "array<array<float, elem>, n_hidden> dL_drZ1;\n",
        "array<array<float, n_features+1>, n_hidden> dL_dW1;\n",
        "array<array<float, n_features+1>, n_hidden> delta_W1_unscaled;\n",
        "array<array<float, n_hidden+1>, n_output> delta_W2_unscaled;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nO22zajGWVv",
        "outputId": "7aef1611-2daa-4f7b-a3cf-56073b749d59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MLP.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA kernel: matrix multiplication using shared memory"
      ],
      "metadata": {
        "id": "e02ZgtMpGamT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel.cu\n",
        "template <int TS> __global__ void tiled_mult(float * __restrict C, float * __restrict A, float * __restrict B,\n",
        "                                             int rowA, int colA, int colB){\n",
        "    __shared__ float Atile[TS][TS];         // tile in A\n",
        "    __shared__ float Btile[TS][TS];         // tile in B\n",
        "\n",
        "    int tx  = threadIdx.x;                  // tile col\n",
        "    int ty  = threadIdx.y;                  // tile row\n",
        "    int ocx = blockDim.x * blockIdx.x;      // tile x origin in C\n",
        "    int ocy = blockDim.y * blockIdx.y;      // tile y origin in C\n",
        "\n",
        "    int ax = tx;                // j in first tile on A\n",
        "    int ay = ocy + ty;          // i in first tile on A and C\n",
        "    int bx = ocx + tx;          // j in first tile on B and C\n",
        "    int by = ty;                // i in first tile on B\n",
        "\n",
        "    float csum = 0.0f;\n",
        "\n",
        "    #pragma unroll\n",
        "    for(int t = 0; t<gridDim.x; t++) {\n",
        "\n",
        "        if (ay < rowA && ax < colA)\n",
        "            Atile[ty][tx] = A[ay*colA + ax];    // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < colA && bx < colB)\n",
        "            Btile[ty][tx] = B[by*colB + bx];    // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        __syncthreads();\n",
        "        for (int k = 0; k<TS; k++) {\n",
        "            csum += Atile[ty][k] * Btile[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        ax += TS;               // step A tiles along rows of A\n",
        "        by += TS;               // step B tiles down cols of B\n",
        "    }\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < rowA && bx < colB)\n",
        "        C[ay * colB + bx] = csum;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "template <int TS> __global__ void tiled_mult_transposeB(float * __restrict C,\n",
        " \t    float * __restrict A, float * __restrict B, int rowA, int colA, int rowB){\n",
        "\t__shared__ float Atile[TS][TS];  // tile in A\n",
        "\t__shared__ float Btile[TS][TS];  // tile in B\n",
        "\n",
        "\tint tx  = threadIdx.x;            // tile col\n",
        "\tint ty  = threadIdx.y;            // tile row\n",
        "\tint ocx = blockDim.x*blockIdx.x;  // tile x origin in C\n",
        "\tint ocy = blockDim.y*blockIdx.y;  // tile y origin in C\n",
        "\n",
        "\tint ax = tx;            // j in first tile on A\n",
        "\tint ay = ocy + ty;      // i in first tile on A and C\n",
        "\tint bx = ocx + tx;      // j in first tile on B\n",
        "\tint by = ty;            // i in first tile on B and C\n",
        "\n",
        "\tfloat csum = 0.0f;\n",
        "\n",
        "    #pragma unroll\n",
        "\tfor(int t=0; t < (colA+TS-1)/TS; t++){\n",
        "        if (ay < rowA && ax < colA)\n",
        "\t\t    Atile[ty][tx] = A[ay*colA + ax];  // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < colA && bx < rowB)\n",
        "\t\t    Btile[ty][tx] = B[bx * colA + by];  // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "\t\t__syncthreads();\n",
        "\t\tfor(int k=0; k<TS; k++){\n",
        "            csum += Atile[ty][k]*Btile[k][tx];\n",
        "        }\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tax += TS;         // step A tiles along rows of A\n",
        "\t\tby += TS;         // step B tiles along rows of B\n",
        "\t}\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < rowA && bx < rowB)\n",
        "\t    C[ay*rowB + bx] = csum;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "template <int TS> __global__ void tiled_mult_transposeA(float * __restrict C, float * __restrict A, float * __restrict B,\n",
        "                                             int rowA, int colA, int colB){\n",
        "\n",
        "    __shared__ float Atile[TS][TS];         // tile in A\n",
        "    __shared__ float Btile[TS][TS];         // tile in B\n",
        "\n",
        "    int tx  = threadIdx.x;                  // tile col\n",
        "    int ty  = threadIdx.y;                  // tile row\n",
        "    int ocx = blockDim.x * blockIdx.x;      // tile x origin in C\n",
        "    int ocy = blockDim.y * blockIdx.y;      // tile y origin in C\n",
        "\n",
        "    int ax = tx;                // j in first tile on A\n",
        "    int ay = ocy + ty;          // i in first tile on A and C\n",
        "    int bx = ocx + tx;          // j in first tile on B and C\n",
        "    int by = ty;                // i in first tile on B\n",
        "\n",
        "    float csum = 0.0f;\n",
        "\n",
        "    #pragma unroll\n",
        "    for(int t = 0; t<gridDim.x; t++) {\n",
        "\n",
        "        if (ay < colA && ax < rowA)\n",
        "            Atile[ty][tx] = A[ax*colA + ay];    // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < colA && bx < colB)\n",
        "            Btile[ty][tx] = B[by*colB + bx];    // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        __syncthreads();\n",
        "        for (int k = 0; k<TS; k++) {\n",
        "            csum += Atile[ty][k] * Btile[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        ax += TS;               // step A tiles along rows of A\n",
        "        by += TS;               // step B tiles down cols of B\n",
        "    }\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < colA && bx < colB)\n",
        "        C[ay * colB + bx] = csum;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBVfHO2gGhq3",
        "outputId": "ee5b14e4-acf6-47f3-99dc-5c5cd577df1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Math function"
      ],
      "metadata": {
        "id": "CSlU0PfkGwFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile math_function.cu\n",
        "#include \"kernel.cu\"\n",
        "\n",
        "void A_mult_B(const float* A, const float* B, float* C,\n",
        "              int rowA, int colA, int colB) {\n",
        "\n",
        "    const unsigned int tilex = 32;  // thread-block x\n",
        "    const unsigned int tiley = 32;   // thread-block y\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rowA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, colA * colB * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, rowA * colB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rowA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, colA * colB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(colB+threads.x-1)/threads.x,(rowA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult<tilex><<<blocks,threads>>>(dev_C, dev_A, dev_B, rowA, colA, colB);\n",
        "\n",
        "    cudaMemcpy(C, dev_C, rowA * colB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "}\n",
        "\n",
        "\n",
        "void A_mult_B_T(const float* A, const float* B, float* C,\n",
        "                   int rowA, int colA, int rowB) {\n",
        "\n",
        "    const unsigned int tilex = 32;  // thread-block x\n",
        "    const unsigned int tiley = 32;   // thread-block y\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rowA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, rowB * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, rowA * rowB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rowA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, rowB * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(rowB+threads.x-1)/threads.x,(rowA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult_transposeB<tilex><<<blocks,threads>>>(dev_C, dev_A, dev_B, rowA, colA, rowB);\n",
        "\n",
        "    cudaMemcpy(C, dev_C, rowA * rowB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "}\n",
        "\n",
        "\n",
        "void A_T_mult_B(const float* A, const float* B, float* C,\n",
        "                int rowA, int colA, int colB) {\n",
        "\n",
        "    const unsigned int tilex = 32;  // thread-block x\n",
        "    const unsigned int tiley = 32;   // thread-block y\n",
        "\n",
        "    float *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void **)&dev_A, rowA * colA * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_B, rowA * colB * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_C, colA * colB * sizeof(float));\n",
        "\n",
        "    // Copy the input matrices from the host to the device\n",
        "    cudaMemcpy(dev_A, A, rowA * colA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, rowA * colB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(colB+threads.x-1)/threads.x,(colA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult_transposeA<tilex><<<blocks,threads>>>(dev_C, dev_A, dev_B, rowA, colA, colB);\n",
        "\n",
        "    cudaMemcpy(C, dev_C, colA * colB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "}\n",
        "\n",
        "\n",
        "void elem_mult_elem(const float* A, const float* B, float* C, int rig, int col) {\n",
        "    for (int i = 0; i < rig; ++i) {\n",
        "        for (int j = 0; j < col; ++j) {\n",
        "            C[i*col+j] = A[i*col+j] * B[i*col+j];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0asp_j79Gx9E",
        "outputId": "c57f4825-df7f-40dc-bd53-0d6c66545797"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing math_function.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "Initialize train and test input and output"
      ],
      "metadata": {
        "id": "F5iPXGhkG0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gen.cu\n",
        "\n",
        "//sinc2D function generation\n",
        "void sinc2D_gen(float* x, float* y, int num_patterns){\n",
        "    int num_points = sqrt(num_patterns);\n",
        "\n",
        "    // linspace x1\n",
        "    vector<float> x1(num_points);\n",
        "    float start_x1 = -5.0;\n",
        "    float end_x1 = 5.0;\n",
        "    float step_x1 = (end_x1 - start_x1) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x1[i] = start_x1 + i * step_x1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // linspace x2\n",
        "    vector<float> x2(num_points);\n",
        "    float start_x2 = -5.0;\n",
        "    float end_x2 = 5.0;\n",
        "    float step_x2 = (end_x2 - start_x2) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x2[i] = start_x2 + i * step_x2;\n",
        "    }\n",
        "\n",
        "\n",
        "    // meshgrid\n",
        "    vector<vector<float>> XX1(num_points, vector<float>(num_points));\n",
        "    vector<vector<float>> XX2(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        for (int j = 0; j < num_points; ++j){\n",
        "            XX1[i][j] = x1[j];\n",
        "            XX2[i][j] = x2[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // sinc2D\n",
        "    vector<vector<float>> YY(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            float sinc_x1 = (XX1[i][j] == 0) ? 1.0 : sin(XX1[i][j]) / XX1[i][j];\n",
        "            float sinc_x2 = (XX2[i][j] == 0) ? 1.0 : sin(XX2[i][j]) / XX2[i][j];\n",
        "            YY[i][j] = 10.0 * sinc_x1 * sinc_x2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // initialization x e y\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            x[(i*num_points+j)*n_features] = XX1[j][i];\n",
        "            x[(i*num_points+j)*n_features + 1] = XX2[j][i];\n",
        "            y[i * num_points + j] = YY[j][i];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWkhMY7rG4lG",
        "outputId": "45841712-3814-4c54-e9ad-d979c4f93dfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gen.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward function\n",
        "Compute the forward step"
      ],
      "metadata": {
        "id": "J6JQJnRxG6iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forward.cu\n",
        "\n",
        "//This function computes the sigmoid function for a scalar, a vector or a matrix\n",
        "array<array<float, elem>, n_hidden> MLP_sigmoid(const array<array<float, elem>, n_hidden> &z){\n",
        "    array<array<float, elem>, n_hidden> sig;\n",
        "\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            sig[i][j] = 1.0 / (1.0 + exp(-z[i][j]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return sig;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// Compute the forward step\n",
        "void MLP_MSELIN_forward(){\n",
        "    // rA0: is the \"reduced A0\" and it coincides with the transpose of the tall input matrix (nObs x nInput)\n",
        "    for (int i = 0; i < elem; ++i) {\n",
        "        for (int j = 0; j < n_features; ++j) {\n",
        "            rA0[j][i] = x_input[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A0  = E(rA0) It is the \"extended\" version of rA0, obtained by it by adding a row of ones as its new first row\n",
        "    // Extend matrix X by adding the bias\n",
        "    for (int i = 0; i < n_features+1; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            a0[i][j] = (i == 0) ? 1 : rA0[i-1][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // rZ1 = \\sum(W1,A0).  It is the pre-activation at layer 1 (the hidden one)\n",
        "    A_mult_B(w1[0].data(), a0[0].data(), rZ1[0].data(), n_hidden, n_features+1, elem);\n",
        "\n",
        "\n",
        "    // rA1 = \\sigma(rZ1).  It is the output of the first layer (the hidden one)\n",
        "    rA1 = MLP_sigmoid(rZ1);\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A1  = E(rA1).       It is the extended version of rA1\n",
        "    // Extend matrix X by adding the bias\n",
        "    for (int i = 0; i < n_hidden+1; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            a1[i][j] = (i == 0) ? 1 : rA1[i-1][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // rZ2 = \\sum(W2,A1).  It is the pre-activation at layer 2 (the output one)\n",
        "    A_mult_B(w2[0].data(), a1[0].data(), rZ2[0].data(), n_output, n_hidden+1, elem);\n",
        "\n",
        "\n",
        "    rA2 = rZ2;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSFpW4V8G_dB",
        "outputId": "2d4ba0e4-8003-4655-dce6-6e212ff30352"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forward.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "Learn weights from training data"
      ],
      "metadata": {
        "id": "htkqzlJ0HOeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.cu\n",
        "#include \"math_function.cu\"\n",
        "#include \"forward.cu\"\n",
        "\n",
        "// initialize weights to Unif[-1,1]\n",
        "void MLP_initialize_weights(){\n",
        "    array<float, n_hidden*(n_features+1)> w1_temp;\n",
        "    for(int i=0; i<n_hidden*(n_features+1); ++i){\n",
        "        w1_temp[i] = 2*(static_cast<float>(rand())/RAND_MAX)-1;\n",
        "    }\n",
        "\n",
        "    // reshape\n",
        "    int index = 0;\n",
        "    for (int j = 0; j < (n_features+1); ++j) {              //col\n",
        "        for (int i = 0; i < n_hidden; ++i) {                //row\n",
        "            w1[i][j] = w1_temp[index++];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    array<float, n_output*(n_hidden+1)> w2_temp;\n",
        "    for(int i=0; i<n_output*(n_hidden+1); ++i){\n",
        "        w2_temp[i] = 2*(static_cast<float>(rand())/RAND_MAX)-1;\n",
        "    }\n",
        "\n",
        "    // reshape\n",
        "    index = 0;\n",
        "    for (int j = 0; j < (n_hidden+1); ++j) {\n",
        "        for (int i = 0; i < n_output; ++i) {\n",
        "            w2[i][j] = w2_temp[index++];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute sigmoid gradient\n",
        "array<array<float, elem>, n_hidden> MLP_sigmoid_gradient(const array<array<float, elem>, n_hidden> &Z) {\n",
        "    array<array<float, elem>, n_hidden> A = MLP_sigmoid(Z);\n",
        "\n",
        "    array<array<float, elem>, n_hidden> sigGrad;\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < elem; ++j) {\n",
        "            sigGrad[i][j] = A[i][j] * (1.0 - A[i][j]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return sigGrad;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "float MLP_MSE_cost(const array<float, elem> &y) {\n",
        "    vector<float> diff(y.size());\n",
        "    for (int i = 0; i < y.size(); ++i) {\n",
        "        diff[i] = (y[i] - rA2[0][i]);\n",
        "        diff[i] *= diff[i];\n",
        "    }\n",
        "\n",
        "\n",
        "    float cost = (accumulate(diff.begin(), diff.end(), 0.0))/ (2.0 * y.size());\n",
        "    return cost;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute the partial derivative of the loss with respect to the two weighting matrices W2 and W1, using the backpropagation algorithm.\n",
        "void MLP_MSELIN_backprop(const array<float, elem> &y){\n",
        "    // rA2 is 1xB\n",
        "    // A1  is (H+1)xB\n",
        "    // A0  is (D+1)xB\n",
        "    // rZ1 is HxB\n",
        "    // y   is 1xB\n",
        "    // W1  is Hx(D+1)\n",
        "    // W2  is 1x(H+1)\n",
        "\n",
        "    // Step 1: compute dL_dZ2 of size 1xB\n",
        "    // NB: rA2 coincides with y_pred\n",
        "    // NB: dL_dZe could be called \"grad2\", the gradient on the output layer with respect the pre-activation Z2\n",
        "    for(int i = 0; i<n_output; i++){\n",
        "        for(int j = 0; j < elem; ++j) {\n",
        "            dL_dZ2[i][j] = rA2[i][j] - y[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // Step 2: compute dL_dW2 % size 1x(H+1)\n",
        "    // NB: dL_dW2 could be called \"delta_W2_unscaled\", because it is of the same size of W2 and stores the unscaled variation\n",
        "    A_mult_B_T(dL_dZ2[0].data(), a1[0].data(), dL_dW2[0].data(), n_output, elem, n_hidden+1);\n",
        "\n",
        "\n",
        "    // Step 3: compute dL_dA1 of size (H+1)xB\n",
        "    A_T_mult_B(w2[0].data(), dL_dZ2[0].data(), dL_dA1[0].data(), n_output, n_hidden+1, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 4: compute dL_drZ1 of size HxB (also sigma_prime_of_rZ1 has size HxB)\n",
        "    // NB: dL_drZ1 could have been called \"grad1\", since it is the gradient at the first layer (the hidden one), with respect to Z1\n",
        "    sigma_prime_of_rZ1 = MLP_sigmoid_gradient(rZ1);\n",
        "    elem_mult_elem(dL_dA1[1].data(), sigma_prime_of_rZ1[0].data(),dL_drZ1[0].data(),  n_hidden, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 5: compute dL_dW1 of size Hx(D+1)\n",
        "    // NB: dL_dW1 could be called \"delta_W1_unscaled\", because it is of the same size of W2 and stores the unscaled variation of W1\n",
        "    A_mult_B_T(dL_drZ1[0].data(), a0[0].data(), dL_dW1[0].data(), n_hidden, elem, n_features+1);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 6: regularise or not\n",
        "    for (int i = 0; i < n_hidden; ++i) {\n",
        "        for (int j = 0; j < n_features+1; ++j) {\n",
        "            delta_W1_unscaled[i][j] = dL_dW1[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int j = 0; j < n_output; ++j) {\n",
        "        for (int i = 0; i < n_hidden+1; ++i) {\n",
        "            delta_W2_unscaled[j][i] = dL_dW2[j][i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    /* -----------------------------------------------------------------------------\n",
        "    NB: grad2 is the gradient at the hidden layer.\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "\n",
        "    NB: grad1 is the gradient at the hidden layer (derivative\n",
        "    of the loss with respect Z1, the pre-activation at the hidden layer).\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "    ----------------------------------------------------------------------------- */\n",
        "}\n",
        "\n",
        "// learn weights from training data\n",
        "void MLP_MSELIN_train(const array<array<float, n_features>, num_train> &x, const array<float, num_train> &y){\n",
        "    // initialize weights w1 and w2\n",
        "    MLP_initialize_weights();\n",
        "\n",
        "    cost.push_back(numeric_limits<float>::infinity());\n",
        "\n",
        "    // loop: epochs\n",
        "    for(int e=1; e<=epochs; e++) {\n",
        "\n",
        "\n",
        "        //reshape\n",
        "        array<array<int, elem>, minibatches> I;\n",
        "        for (int i = 0; i < num_train; ++i) {\n",
        "            int row = i % minibatches;\n",
        "            int col = i / minibatches;\n",
        "            I[row][col] = i;\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        // loop: minibatches\n",
        "        for(int m=1; m<=minibatches; ++m){\n",
        "            array<int, elem> idx = I[m-1];\n",
        "\n",
        "            // Compute the forward step\n",
        "            for(int i=0; i<elem; i++) {\n",
        "                copy(x[idx[i]].begin(), x[idx[i]].end(), x_input[i].begin());\n",
        "            }\n",
        "            // Feedforward\n",
        "            MLP_MSELIN_forward();\n",
        "\n",
        "\n",
        "            // Compute cost function\n",
        "            array<float, elem> y_index;\n",
        "            for(int i=0; i<elem; i++) {\n",
        "                y_index[i] = y[idx[i]];\n",
        "            }\n",
        "            float step_cost = MLP_MSE_cost(y_index);\n",
        "            cost.push_back(step_cost);\n",
        "\n",
        "\n",
        "            printf(\"Epoch %d/%d, minibatch %04d, Loss (MSE) %g\\n\", e, epochs, m, step_cost);\n",
        "\n",
        "\n",
        "            // Compute gradient via backpropagation\n",
        "            MLP_MSELIN_backprop(y_index);\n",
        "\n",
        "\n",
        "            array<array<float, n_features+1>, n_hidden> delta_W1;\n",
        "            for (int i = 0; i < n_hidden; ++i) {\n",
        "                for (int j = 0; j < n_features+1; ++j) {\n",
        "                    delta_W1[i][j] = eta * delta_W1_unscaled[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "            array<array<float, n_hidden+1>, n_output> delta_W2;\n",
        "            for (int i = 0; i < n_output; ++i) {\n",
        "                for (int j = 0; j < n_hidden+1; ++j) {\n",
        "                    delta_W2[i][j] = eta * delta_W2_unscaled[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "            for (int i = 0; i < n_hidden; ++i) {\n",
        "                for (int j = 0; j < n_features+1; ++j) {\n",
        "                    w1[i][j] -= delta_W1[i][j];\n",
        "                }\n",
        "            }\n",
        "\n",
        "            for (int i = 0; i < n_output; ++i) {\n",
        "                for (int j = 0; j < n_hidden+1; ++j) {\n",
        "                    w2[i][j] -= delta_W2[i][j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKRedPCkHSEd",
        "outputId": "47b0356b-0a13-4360-99fc-bca4183fa74f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ],
      "metadata": {
        "id": "aUaI6t77HYKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.cu\n",
        "\n",
        "// Predict the outputs for all the observations in X, where each row of X is a distinct observation.\n",
        "void MLP_MSELIN_predict(float* x, float* y, int tot_elem) {\n",
        "    for (int i = 0; i < tot_elem; i += elem) {\n",
        "\n",
        "        for (int k = 0; k < elem * n_features; ++k) {\n",
        "            int row = k / n_features;\n",
        "            int col = k % n_features;\n",
        "            x_input[row][col] = x[i * n_features + k];\n",
        "        }\n",
        "\n",
        "        // Feedforward\n",
        "        MLP_MSELIN_forward();\n",
        "\n",
        "        // Copia dei risultati nel vettore y\n",
        "        copy(rA2[0].begin(), rA2[0].begin() + elem, y + i);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvrvWvrZHh-m",
        "outputId": "d5cc03d7-1983-46f7-982d-bc365fe50ed3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main"
      ],
      "metadata": {
        "id": "2pjvQOMSHk6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"MLP.h\"\n",
        "#include \"gen.cu\"\n",
        "#include \"train.cu\"\n",
        "#include \"predict.cu\"\n",
        "\n",
        "int main(){\n",
        "    array<array<float, n_features>, num_train> x_train;\n",
        "    array<float, num_train> y_train;\n",
        "    sinc2D_gen(x_train[0].data(), y_train.data(), num_train);\n",
        "\n",
        "    array<array<float, n_features>, num_test> x_test;\n",
        "    array<float, num_test> y_test;\n",
        "    sinc2D_gen(x_test[0].data(), y_test.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Shuffling training data\n",
        "    array<int, num_train> shuffled_ind;\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        shuffled_ind[i] = i;\n",
        "    }\n",
        "\n",
        "    default_random_engine generator(std::time(nullptr));\n",
        "    shuffle(shuffled_ind.begin(), shuffled_ind.end(), generator);\n",
        "\n",
        "    array<array<float, n_features>, num_train> x_train_temp;\n",
        "    array<float, num_train> y_train_temp;\n",
        "\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        x_train_temp[i] = x_train[shuffled_ind[i]];\n",
        "        y_train_temp[i] = y_train[shuffled_ind[i]];\n",
        "    }\n",
        "\n",
        "    x_train = x_train_temp;\n",
        "    y_train = y_train_temp;\n",
        "\n",
        "    // Learn weights from training data\n",
        "    MLP_MSELIN_train(x_train, y_train);\n",
        "\n",
        "\n",
        "\n",
        "    // Predict the outputs for all the observations in X\n",
        "    array<float, num_train> ytrain_pred;\n",
        "    MLP_MSELIN_predict(x_train[0].data(), ytrain_pred.data(), num_train);\n",
        "\n",
        "    array<float, num_test> ytest_pred;\n",
        "    MLP_MSELIN_predict(x_test[0].data(), ytest_pred.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Compute accuracy (MSE)\n",
        "    float acc_train = 0.0;\n",
        "    for (int i = 0; i < y_train.size(); ++i) {\n",
        "        acc_train += (y_train[i] - ytrain_pred[i])*(y_train[i] - ytrain_pred[i]);\n",
        "    }\n",
        "    acc_train /= (2 * y_train.size());\n",
        "    printf(\"Training accuracy (MSE): %g\\n\", acc_train);\n",
        "\n",
        "    float acc_test = 0.0;\n",
        "    for (int i = 0; i < y_test.size(); ++i) {\n",
        "        acc_test += (y_test[i] - ytest_pred[i])*(y_test[i] - ytest_pred[i]);\n",
        "    }\n",
        "    acc_test /= (2 * y_test.size());\n",
        "    printf(\"Test accuracy: (MSE): %g\\n\", acc_test);\n",
        "\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6anHHCa5Hpbz",
        "outputId": "ff41e81b-0cb8-4e85-ab8d-1a21b872f0fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run"
      ],
      "metadata": {
        "id": "DnCIhYU0Hsdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm prog\n",
        "!nvcc -o prog main.cu"
      ],
      "metadata": {
        "id": "MuBzO9-UHvPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./prog"
      ],
      "metadata": {
        "id": "VWk-72uiHxcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
