{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjKGOy6d2GLaCx1jbpK5MA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabochicchio02/MLP_in_CUDA/blob/main/MLP_sinc2D_tiled_mult.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Layer Perceptron (MLP) for REGRESSION**"
      ],
      "metadata": {
        "id": "3oWxHT5YGIfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer Perceptron to solve a regression problem (sinc2D function) implemented in CUDA using **GPU shared memory for multiplication**"
      ],
      "metadata": {
        "id": "Qy20u5BVGMEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU su COLAB:** Runtime > Cambia tipo di runtime > GPU > Salva"
      ],
      "metadata": {
        "id": "rR4k56CaGPQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Including libraries and initializing MLP parameters"
      ],
      "metadata": {
        "id": "QZKXC_U3GSzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MLP.cuh\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <array>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <random>\n",
        "#include <ctime>\n",
        "#include <limits>\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "\n",
        "\n",
        "const int num_train = 150*150;              // number of training pattern (put a square number here)\n",
        "const int num_test = 2500;\n",
        "\n",
        "\n",
        "// //////////////////////////////////////////// //\n",
        "//                 MLP parameters               //\n",
        "// //////////////////////////////////////////// //\n",
        "const int n_output = 1;                     // Number of outputs\n",
        "const int n_features = 2;                   // Number of input features\n",
        "const int n_hidden = 300;                   // Number of neurons in the hidden layer\n",
        "const int epochs = 500;                     // Number of epochs\n",
        "const int minibatches = 30;                 // Number of mini-batches\n",
        "__device__ float eta = 1e-6;                           // Learning rate\n",
        "\n",
        "\n",
        "// WEIGHTS MODEL AND BIAS\n",
        "array<array<float, n_features+1>, n_hidden> w1 = {};\n",
        "array<array<float, n_hidden+1>, n_output> w2 = {};\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// Global declaration of variable used in the train step\n",
        "const int elem = (num_train + minibatches -1 )/minibatches;     // number of inputs used in each minibatch\n",
        "\n",
        "\n",
        "// Threads kernel CUDA\n",
        "const int tilex = 32;    // thread-block x\n",
        "const int tiley = 32;    // thread-block y\n",
        "\n",
        "// GPU DATA\n",
        "float* dev_x_train;         // dataset train\n",
        "float* dev_y_train;         // dataset train\n",
        "float* dev_w1;              // pesi w1\n",
        "float* dev_w2;              // pesi  w2\n",
        "float* dev_w2_extend;       // column vector to matrix\n",
        "int* dev_I;                 // matrice per indice dataset\n",
        "\n",
        "//forward\n",
        "float* dev_x_input;         // minibatch ingresso MLP\n",
        "float* dev_y_input;         // minibatch ingresso MLP\n",
        "float* dev_a0;              // input and bias\n",
        "float* dev_rZ1;             // ris somma pesata\n",
        "float* dev_rA1;             // uscita hidden layer\n",
        "float* dev_a1;              // ingresso output layer\n",
        "float* dev_rZ2;             // ris somma pesata output layer\n",
        "float* dev_rA2;             // uscita MLP\n",
        "\n",
        "float* dev_square_error;\n",
        "\n",
        "//backpropagation\n",
        "float* dev_dL_dZ2;\n",
        "float* dev_dL_dZ2_extend;   // column vector to matrix\n",
        "float* dev_dL_dW2;\n",
        "float* dev_dL_dA1;\n",
        "float* dev_sigma_prime;\n",
        "float* dev_dL_drZ1;\n",
        "float* dev_dL_dW1;\n",
        "\n",
        "float* dev_delta_W1_unscaled;\n",
        "float* dev_delta_W2_unscaled;\n",
        "\n",
        "// predict\n",
        "float* dev_x_pred;            // dataset train in predict (inizialization in predict)\n",
        "float* dev_y_pred;            // dataset train in predict (inizialization in predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nO22zajGWVv",
        "outputId": "ea8c3f72-b70a-4171-f734-afd738d592a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MLP.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU allocation"
      ],
      "metadata": {
        "id": "obIpL54rYFMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile allocation.cu\n",
        "\n",
        "void GPU_alloc(){\n",
        "    cudaMalloc((void **)&dev_x_train, num_train * n_features * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_w1, n_hidden * (n_features+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_w2, n_output * (n_hidden+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_w2_extend, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_y_train, num_train * n_output * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_I, minibatches * elem * sizeof(int));\n",
        "    cudaMalloc((void **)&dev_x_input, elem * n_features * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_y_input, elem * n_output * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_a0, (n_features+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rZ1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rA1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_a1, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rZ2, n_output * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_rA2, n_output * elem * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_square_error, elem * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_dL_dZ2, n_output * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dZ2_extend, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dW2, n_output * (n_hidden+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dA1, (n_hidden+1) * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_sigma_prime, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_drZ1, n_hidden * elem * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_dL_dW1, n_hidden * (n_features+1) * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void **)&dev_delta_W1_unscaled, n_hidden * (n_features+1) * sizeof(float));\n",
        "    cudaMalloc((void **)&dev_delta_W2_unscaled, n_output * (n_hidden+1) * sizeof(float));\n",
        "}\n",
        "\n",
        "\n",
        "void GPU_free(){\n",
        "    cudaFree(dev_x_train);\n",
        "    cudaFree(dev_w1);\n",
        "    cudaFree(dev_w2);\n",
        "    cudaFree(dev_y_train);\n",
        "\n",
        "    cudaFree(dev_I);\n",
        "    cudaFree(dev_x_input);\n",
        "    cudaFree(dev_y_input);\n",
        "    cudaFree(dev_a0);\n",
        "    cudaFree(dev_rZ1);\n",
        "    cudaFree(dev_rA1);\n",
        "    cudaFree(dev_a1);\n",
        "    cudaFree(dev_rZ2);\n",
        "    cudaFree(dev_rA2);\n",
        "\n",
        "    cudaFree(dev_square_error);\n",
        "\n",
        "    cudaFree(dev_dL_dZ2);\n",
        "    cudaFree(dev_dL_dW2);\n",
        "    cudaFree(dev_dL_dA1);\n",
        "    cudaFree(dev_sigma_prime);\n",
        "    cudaFree(dev_dL_drZ1);\n",
        "    cudaFree(dev_dL_dW1);\n",
        "\n",
        "    cudaFree(dev_delta_W1_unscaled);\n",
        "    cudaFree(dev_delta_W2_unscaled);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duLK59IzYEzg",
        "outputId": "242ed7d9-f011-4c86-f3d5-28020e8f1c7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing allocation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA kernel"
      ],
      "metadata": {
        "id": "e02ZgtMpGamT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel.cu\n",
        "\n",
        "template <int TS> __global__ void tiled_mult(float * __restrict C, const float * __restrict A, const float * __restrict B,\n",
        "                                             int rowA, int colA, int colB)\n",
        "{\n",
        "    __shared__ float Atile[TS][TS];         // tile in A\n",
        "    __shared__ float Btile[TS][TS];         // tile in B\n",
        "\n",
        "    int tx  = threadIdx.x;                  // tile col\n",
        "    int ty  = threadIdx.y;                  // tile row\n",
        "    int ocx = blockDim.x * blockIdx.x;      // tile x origin in C\n",
        "    int ocy = blockDim.y * blockIdx.y;      // tile y origin in C\n",
        "\n",
        "    int ay = ocy + ty;          // i in first tile on A and C\n",
        "    int bx = ocx + tx;          // j in first tile on B and C\n",
        "\n",
        "    float csum = 0.0f;\n",
        "\n",
        "    int numTiles = (colA + TS - 1) / TS;\n",
        "\n",
        "    #pragma unroll\n",
        "    for(int t = 0; t < numTiles; t++) {\n",
        "        int ax = t * TS + tx;    // current column in A\n",
        "        int by = t * TS + ty;    // current row in B\n",
        "\n",
        "\n",
        "        if (ay < rowA && ax < colA)\n",
        "            Atile[ty][tx] = A[ay*colA + ax];    // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < colA && bx < colB)\n",
        "            Btile[ty][tx] = B[by*colB + bx];    // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k<TS; k++) {\n",
        "                csum += Atile[ty][k] * Btile[k][tx];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        ax += TS;               // step A tiles along rows of A\n",
        "        by += TS;               // step B tiles down cols of B\n",
        "    }\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < rowA && bx < colB)\n",
        "        C[ay * colB + bx] = csum;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "template <int TS> __global__ void tiled_mult_transposeB(float * __restrict C,\n",
        " \t    const float * __restrict A, const float * __restrict B, int rowA, int colA, int rowB)\n",
        "{\n",
        "\t__shared__ float Atile[TS][TS];  // tile in A\n",
        "\t__shared__ float Btile[TS][TS];  // tile in B\n",
        "\n",
        "\tint tx  = threadIdx.x;            // tile col\n",
        "\tint ty  = threadIdx.y;            // tile row\n",
        "\tint ocx = blockDim.x*blockIdx.x;  // tile x origin in C\n",
        "\tint ocy = blockDim.y*blockIdx.y;  // tile y origin in C\n",
        "\n",
        "\n",
        "\tint ay = ocy + ty;      // i in first tile on A and C\n",
        "\tint bx = ocx + tx;      // j in first tile on B\n",
        "\n",
        "\tfloat csum = 0.0f;\n",
        "\n",
        "    int numTiles = (colA + TS - 1) / TS;\n",
        "\n",
        "    #pragma unroll\n",
        "    for(int t = 0; t < numTiles; t++) {\n",
        "        int ax = t * TS + tx;    // current column in A\n",
        "        int by = t * TS + ty;    // current row in B\n",
        "\n",
        "        if (ay < rowA && ax < colA)\n",
        "\t\t    Atile[ty][tx] = A[ay*colA + ax];  // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < colA && bx < rowB)\n",
        "\t\t    Btile[ty][tx] = B[bx * colA + by];  // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "\t\t__syncthreads();\n",
        "\t\tfor(int k=0; k<TS; k++){\n",
        "            csum += Atile[ty][k]*Btile[k][tx];\n",
        "        }\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tax += TS;         // step A tiles along rows of A\n",
        "\t\tby += TS;         // step B tiles along rows of B\n",
        "\t}\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < rowA && bx < rowB)\n",
        "\t    C[ay*rowB + bx] = csum;\n",
        "}\n",
        "\n",
        "template <int TS> __global__ void tiled_mult_transposeA(float * __restrict C, const float * __restrict A, const float * __restrict B,\n",
        "                                             int rowA, int colA, int colB)\n",
        "{\n",
        "    __shared__ float Atile[TS][TS];         // tile in A\n",
        "    __shared__ float Btile[TS][TS];         // tile in B\n",
        "\n",
        "    int tx  = threadIdx.x;                  // tile col\n",
        "    int ty  = threadIdx.y;                  // tile row\n",
        "    int ocx = blockDim.x * blockIdx.x;      // tile x origin in C\n",
        "    int ocy = blockDim.y * blockIdx.y;      // tile y origin in C\n",
        "\n",
        "    int ay = ocy + ty;          // i in first tile on A and C\n",
        "    int bx = ocx + tx;          // j in first tile on B and C\n",
        "\n",
        "    float csum = 0.0f;\n",
        "\n",
        "    int numTiles = (rowA + TS - 1) / TS;\n",
        "\n",
        "    #pragma unroll\n",
        "        for(int t = 0; t < numTiles; t++) {\n",
        "        int ax = t * TS + tx;    // current column in A\n",
        "        int by = t * TS + ty;    // current row in B\n",
        "\n",
        "        if (ay < colA && ax < rowA)\n",
        "            Atile[ty][tx] = A[ax * colA + ay];    // copy A tile to shared mem\n",
        "        else\n",
        "            Atile[ty][tx] = 0.0f;\n",
        "\n",
        "\n",
        "        if (by < rowA && bx < colB)\n",
        "            Btile[ty][tx] = B[by * colB + bx];    // copy B tile to shared mem\n",
        "        else\n",
        "            Btile[ty][tx] = 0.0f;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < TS; k++) {\n",
        "            csum += Atile[ty][k] * Btile[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        ax += TS;               // step A tiles along rows of A\n",
        "        by += TS;               // step B tiles down cols of B\n",
        "    }\n",
        "\n",
        "    // store complete result\n",
        "    if (ay < colA && bx < colB)\n",
        "        C[ay * colB + bx] = csum;\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void setMinibatchIdx(int* dev_I, int minibatches, int elem) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\n",
        "    if (idx_x < elem && idx_y < minibatches){\n",
        "        int i = idx_y * elem + idx_x;\n",
        "\n",
        "        int row = i % minibatches;\n",
        "        int col = i / minibatches;\n",
        "\n",
        "        dev_I[row * elem + col] = i;                // set index\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void setBatch(float* x, float* x_input, int row, int col, int m) {\n",
        "    int tid_x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int tid_y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "    if (tid_x < col && tid_y < row) {\n",
        "        int idx = tid_y + (m - 1) * row;\n",
        "        int x_idx = idx * col + tid_x;\n",
        "        int x_input_idx = tid_y * col + tid_x;\n",
        "\n",
        "        x_input[x_input_idx] = x[x_idx];      // Batch copy of data for MLP input\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// Transpose matrix and set first row to 1\n",
        "__global__ void transpose_extend(float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x <= col) {\n",
        "        output[idx_x * row + idx_y] = (idx_x == 0) ? 1.0f : input[idx_y * col + (idx_x-1)];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void func_sigmoid(float* output, const float* input, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = 1.0f / (1.0f + expf(-input[idx]));\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sigmoid_gradient(float* output, const float* rA1, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = rA1[idx] * (1.0 - rA1[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void extend_1(float* output, float* input, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x <= col) {\n",
        "        output[idx_y * col + idx_x] = (idx_y == 0) ? 1.0f : input[(idx_y - 1) * col + idx_x];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void elem_mult_elem(float* A, const float* B, float* C, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        C[idx] = A[idx] * B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void elem_sub(float* A, const float* B, float* C, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        C[idx] = A[idx] - B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void copy(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] = input[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void update_weights(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "\n",
        "        output[idx] -= eta * input[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void square_error(float* A, const float* B, float* C, int n_hidden, int elem) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    int idx = idx_y * elem + idx_x;\n",
        "\n",
        "    if (idx_y < n_hidden && idx_x < elem) {\n",
        "        float diff = A[idx] - B[idx];\n",
        "        C[idx] = diff * diff;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void initialize_weights(float *W, curandState *states, long seed, int rows, int cols) {\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int idx = idx_y * cols + idx_x;\n",
        "\n",
        "    if (idx_y < rows && idx_x < cols) {\n",
        "        curand_init(seed+idx, 0, 0, &states[idx]);\n",
        "\n",
        "        curandState localState = states[idx];\n",
        "        W[idx] = 2.0f * curand_uniform(&localState) - 1.0f;\n",
        "        states[idx] = localState;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void reduce(float *y, float *x, int N){\n",
        "    extern __shared__ float tsum[];\n",
        "\n",
        "    int id = threadIdx.x;\n",
        "    int tid = blockDim.x*blockIdx.x+threadIdx.x;\n",
        "    int stride =  gridDim.x*blockDim.x;\n",
        "\n",
        "    tsum[id] = 0.0f;\n",
        "    for(int k=tid;k<N;k+=stride)\n",
        "        tsum[id] += x[k];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // number of blocks power of 2 greater than blockDim\n",
        "    int block2 = 1;\n",
        "    while (block2 < blockDim.x) block2 *= 2;\n",
        "\n",
        "    // power of 2 reduction loop\n",
        "    for(int k=block2/2; k>0; k >>= 1){\n",
        "        if(id<k && id+k < blockDim.x)\n",
        "            tsum[id] += tsum[id+k];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if(id==0) y[blockIdx.x] = tsum[0];\n",
        "}\n",
        "\n",
        "__global__ void transpose_extend_row_vector(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;\n",
        "        output[idx] = input[idx_y];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void extend_row_vector(const float* input, float* output, int row, int col) {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    // Calcola l'indice lineare dell'elemento nella matrice\n",
        "    if (idx_y < row && idx_x < col) {\n",
        "        int idx = idx_y * col + idx_x;  // Accesso in riga (row-major)\n",
        "        output[idx] = input[idx_x];\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBVfHO2gGhq3",
        "outputId": "2f2fcb54-ffae-4632-d0ce-755b4defbbf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Math function"
      ],
      "metadata": {
        "id": "CSlU0PfkGwFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile math_function.cu\n",
        "#include \"kernel.cu\"\n",
        "\n",
        "void A_mult_B(const float* A, const float* B, float* C,\n",
        "              int rowA, int colA, int colB) {\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(colB+threads.x-1)/threads.x,(rowA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult<tilex><<<blocks,threads>>>(C, A, B, rowA, colA, colB);\n",
        "}\n",
        "\n",
        "\n",
        "void A_mult_B_T(const float* A, const float* B, float* C,\n",
        "                   int rowA, int colA, int rowB) {\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(rowB+threads.x-1)/threads.x,(rowA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult_transposeB<tilex><<<blocks,threads>>>(C, A, B, rowA, colA, rowB);\n",
        "}\n",
        "\n",
        "void A_T_mult_B(const float* A, const float* B, float* C,\n",
        "                int rowA, int colA, int colB) {\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(colB+threads.x-1)/threads.x,(colA+threads.y-1)/threads.y,1};\n",
        "\n",
        "    tiled_mult_transposeA<tilex><<<blocks,threads>>>(C, A, B, rowA, colA, colB);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0asp_j79Gx9E",
        "outputId": "6d0f36f7-0ba7-40fa-d33d-b1fbe1359e00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing math_function.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "Initialize train and test input and output"
      ],
      "metadata": {
        "id": "F5iPXGhkG0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gen.cu\n",
        "\n",
        "//sinc2D function generation\n",
        "void sinc2D_gen(float* x, float* y, int num_patterns){\n",
        "    int num_points = sqrt(num_patterns);\n",
        "\n",
        "    // linspace x1\n",
        "    vector<float> x1(num_points);\n",
        "    float start_x1 = -5.0;\n",
        "    float end_x1 = 5.0;\n",
        "    float step_x1 = (end_x1 - start_x1) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x1[i] = start_x1 + i * step_x1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // linspace x2\n",
        "    vector<float> x2(num_points);\n",
        "    float start_x2 = -5.0;\n",
        "    float end_x2 = 5.0;\n",
        "    float step_x2 = (end_x2 - start_x2) / (num_points - 1);\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        x2[i] = start_x2 + i * step_x2;\n",
        "    }\n",
        "\n",
        "\n",
        "    // meshgrid\n",
        "    vector<vector<float>> XX1(num_points, vector<float>(num_points));\n",
        "    vector<vector<float>> XX2(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i){\n",
        "        for (int j = 0; j < num_points; ++j){\n",
        "            XX1[i][j] = x1[j];\n",
        "            XX2[i][j] = x2[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // sinc2D\n",
        "    vector<vector<float>> YY(num_points, vector<float>(num_points));\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            float sinc_x1 = (XX1[i][j] == 0) ? 1.0 : sin(XX1[i][j]) / XX1[i][j];\n",
        "            float sinc_x2 = (XX2[i][j] == 0) ? 1.0 : sin(XX2[i][j]) / XX2[i][j];\n",
        "            YY[i][j] = 10.0 * sinc_x1 * sinc_x2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // initialization x e y\n",
        "    for (int i = 0; i < num_points; ++i) {\n",
        "        for (int j = 0; j < num_points; ++j) {\n",
        "            x[(i*num_points+j)*n_features] = XX1[j][i];\n",
        "            x[(i*num_points+j)*n_features + 1] = XX2[j][i];\n",
        "            y[i * num_points + j] = YY[j][i];\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWkhMY7rG4lG",
        "outputId": "0f915e75-761e-48ce-c35a-66e795184f9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gen.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forward function\n",
        "Compute the forward step"
      ],
      "metadata": {
        "id": "J6JQJnRxG6iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forward.cu\n",
        "\n",
        "// Compute the forward step\n",
        "void MLP_MSELIN_forward(int elem){\n",
        "    // rA0: is the \"reduced A0\" and it coincides with the transpose of the tall input matrix (nObs x nInput)\n",
        "    // A0  = E(rA0) It is the \"extended\" version of rA0, obtained by it by adding a row of ones as its new first row\n",
        "    // Extend matrix X by adding the bias\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={(n_features+threads.x-1)/threads.x, (elem+threads.y-1)/threads.y, 1};\n",
        "    transpose_extend<<<blocks, threads>>>(dev_x_input, dev_a0, elem, n_features);\n",
        "\n",
        "\n",
        "    // rZ1 = \\sum(W1,A0).  It is the pre-activation at layer 1 (the hidden one)\n",
        "    A_mult_B(dev_w1, dev_a0, dev_rZ1, n_hidden, n_features+1, elem);\n",
        "\n",
        "\n",
        "    // rA1 = \\sigma(rZ1).  It is the output of the first layer (the hidden one)\n",
        "    blocks ={(elem+threads.x-1)/threads.x, (n_hidden+threads.y-1)/threads.y, 1};\n",
        "    func_sigmoid<<<blocks, threads>>>(dev_rA1, dev_rZ1, n_hidden, elem);\n",
        "\n",
        "\n",
        "    // MLP_extend\n",
        "    // A1  = E(rA1).       It is the extended version of rA1\n",
        "    // Extend matrix X by adding the bias\n",
        "    blocks ={(elem+threads.x-1)/threads.x, ((n_hidden+1)+threads.y-1)/threads.y, 1};\n",
        "    extend_1<<<blocks, threads>>>(dev_a1, dev_rA1, (n_hidden+1), elem);\n",
        "\n",
        "\n",
        "    // rZ2 = \\sum(W2,A1).  It is the pre-activation at layer 2 (the output one)\n",
        "    A_mult_B(dev_w2, dev_a1, dev_rZ2, n_output, n_hidden+1, elem);\n",
        "\n",
        "    dev_rA2 = dev_rZ2;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSFpW4V8G_dB",
        "outputId": "8c3c3528-4510-4677-f80b-65f4224526bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forward.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "Learn weights from training data"
      ],
      "metadata": {
        "id": "htkqzlJ0HOeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.cu\n",
        "#include \"math_function.cu\"\n",
        "#include \"forward.cu\"\n",
        "#include \"allocation.cu\"\n",
        "\n",
        "float MLP_MSE_cost(int elem) {\n",
        "    // Calculate the differences between y and rA2\n",
        "    dim3 threads = {tilex, 1, 1};\n",
        "    dim3 blocks ={(elem + threads.x-1)/threads.x,1,1};\n",
        "    square_error<<<blocks, threads>>>(dev_rA2, dev_y_input, dev_square_error, n_output, elem);\n",
        "\n",
        "\n",
        "    float* dev_temp_sum;        //partial sum of block\n",
        "    cudaMalloc((void **)&dev_temp_sum, blocks.x * sizeof(float));\n",
        "\n",
        "    //partial sum for each block\n",
        "    reduce<<<blocks,threads, threads.x*sizeof(float)>>>(dev_temp_sum, dev_square_error, elem);      //somma parziale ogni blocco\n",
        "\n",
        "    int blocchi = blocks.x;\n",
        "    reduce<<<1, blocks.x, blocks.x * sizeof(float)>>>(dev_square_error, dev_temp_sum, blocchi);\n",
        "\n",
        "    float res;\n",
        "    cudaMemcpy(&res, &dev_square_error[0], sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(dev_temp_sum);\n",
        "\n",
        "    return res;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// Compute the partial derivative of the loss with respect to the two weighting matrices W2 and W1, using the backpropagation algorithm.\n",
        "void MLP_MSELIN_backprop(){\n",
        "    // rA2 is 1xB\n",
        "    // A1  is (H+1)xB\n",
        "    // A0  is (D+1)xB\n",
        "    // rZ1 is HxB\n",
        "    // y   is 1xB\n",
        "    // W1  is Hx(D+1)\n",
        "    // W2  is 1x(H+1)\n",
        "\n",
        "    // Step 1: compute dL_dZ2 of size 1xB\n",
        "    // NB: rA2 coincides with y_pred\n",
        "    // NB: dL_dZe could be called \"grad2\", the gradient on the output layer with respect the pre-activation Z2\n",
        "\n",
        "    dim3 threads = {tilex, 1, 1};\n",
        "    dim3 blocks ={(elem + threads.x-1)/threads.x,1,1};\n",
        "    elem_sub<<<blocks, threads>>>(dev_rA2, dev_y_input, dev_dL_dZ2, n_output, elem);\n",
        "\n",
        "\n",
        "    // Step 2: compute dL_dW2 % size 1x(H+1)\n",
        "    // NB: dL_dW2 could be called \"delta_W2_unscaled\", because it is of the same size of W2 and stores the unscaled variation\n",
        "    A_mult_B_T(dev_dL_dZ2, dev_a1, dev_dL_dW2, n_output, elem, n_hidden+1);\n",
        "\n",
        "    // Step 3: compute dL_dA1 of size (H+1)xB\n",
        "    threads = {tilex, tiley, 1};\n",
        "    blocks ={(elem + threads.x-1)/threads.x,((n_hidden+1)+threads.y-1)/threads.y,1};\n",
        "    transpose_extend_row_vector<<<blocks, threads>>>(dev_w2, dev_w2_extend, (n_hidden+1), elem);\n",
        "    extend_row_vector<<<blocks, threads>>>(dev_dL_dZ2, dev_dL_dZ2_extend, (n_hidden+1), elem);\n",
        "    elem_mult_elem<<<blocks, threads>>>(dev_w2_extend, dev_dL_dZ2_extend, dev_dL_dA1, (n_hidden+1), elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 4: compute dL_drZ1 of size HxB (also sigma_prime_of_rZ1 has size HxB)\n",
        "    // NB: dL_drZ1 could have been called \"grad1\", since it is the gradient at the first layer (the hidden one), with respect to Z1\n",
        "    threads = {tilex, tiley, 1};\n",
        "    blocks ={(elem + threads.x-1)/threads.x,(n_hidden+threads.y-1)/threads.y,1};\n",
        "    sigmoid_gradient<<<blocks, threads>>>(dev_sigma_prime, dev_rA1, n_hidden, elem);\n",
        "    elem_mult_elem<<<blocks, threads>>>(&dev_dL_dA1[elem], dev_sigma_prime, dev_dL_drZ1, n_hidden, elem);\n",
        "\n",
        "\n",
        "\n",
        "    // Step 5: compute dL_dW1 of size Hx(D+1)\n",
        "    // NB: dL_dW1 could be called \"delta_W1_unscaled\", because it is of the same size of W2 and stores the unscaled variation of W1\n",
        "    A_mult_B_T(dev_dL_drZ1, dev_a0, dev_dL_dW1, n_hidden, elem, n_features+1);\n",
        "\n",
        "\n",
        "    blocks ={((n_features+1) + threads.x-1)/threads.x,(n_hidden+threads.y-1)/threads.y,1};\n",
        "    copy<<<blocks, threads>>>(dev_dL_dW1, dev_delta_W1_unscaled, n_hidden, (n_features+1));\n",
        "\n",
        "\n",
        "    blocks ={((n_hidden+1) + threads.x-1)/threads.x,(n_output+threads.y-1)/threads.y,1};\n",
        "    copy<<<blocks, threads>>>(dev_dL_dW2, dev_delta_W2_unscaled, n_output, (n_hidden+1));\n",
        "\n",
        "\n",
        "    /* -----------------------------------------------------------------------------\n",
        "    NB: grad2 is the gradient at the hidden layer.\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "\n",
        "    NB: grad1 is the gradient at the hidden layer (derivative\n",
        "    of the loss with respect Z1, the pre-activation at the hidden layer).\n",
        "    It is a column vector in the case of a single pattern\n",
        "    (minibatch equal to the training set site) or a matrix,\n",
        "    to be imagined, in the latter case, a matrix of columns,\n",
        "    the gradients of each input pattern in the minibatch.\n",
        "    ----------------------------------------------------------------------------- */\n",
        "}\n",
        "\n",
        "\n",
        "// learn weights from training data\n",
        "void MLP_MSELIN_train(const array<array<float, n_features>, num_train> &x, const array<float, num_train> &y){\n",
        "    GPU_alloc();\n",
        "\n",
        "    // initialize weights w1 and w2\n",
        "    random_device rd;\n",
        "    long seed = rd();\n",
        "\n",
        "    curandState *states_w1;         //required for cuRAND\n",
        "    cudaMalloc(&states_w1, n_hidden * (n_features+1) * sizeof(curandState));\n",
        "\n",
        "    dim3 threads ={tilex,tiley,1};\n",
        "    dim3 blocks ={((n_features+1) + threads.x-1)/threads.x, (n_hidden+threads.y-1)/threads.y,1};\n",
        "    initialize_weights<<<blocks, threads>>>(dev_w1, states_w1, seed, n_hidden, (n_features+1));\n",
        "\n",
        "    curandState *states_w2;         //required for cuRAND\n",
        "    cudaMalloc(&states_w2, n_output * (n_hidden+1) * sizeof(curandState));\n",
        "\n",
        "    blocks ={((n_hidden+1) + threads.x-1)/threads.x, (n_output+threads.y-1)/threads.y,1};\n",
        "    initialize_weights<<<blocks, threads>>>(dev_w2, states_w2, seed, n_output, (n_hidden+1));\n",
        "\n",
        "    cudaFree(states_w1);\n",
        "    cudaFree(states_w2);\n",
        "\n",
        "\n",
        "\n",
        "    // COPIA DATASET GPU\n",
        "    cudaMemcpy(dev_x_train, &x[0][0], num_train * n_features * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_y_train, &y[0], num_train * n_output * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    blocks ={(elem + threads.x-1)/threads.x,(minibatches+threads.y-1)/threads.y,1};\n",
        "    setMinibatchIdx<<<blocks, threads>>>(dev_I, minibatches, elem);\n",
        "\n",
        "    // loop: epochs\n",
        "    for(int e=1; e<=epochs; ++e) {\n",
        "\n",
        "        // loop: minibatches\n",
        "        for(int m=1; m<=minibatches; ++m){\n",
        "\n",
        "            // input for this minibatch\n",
        "            blocks ={(n_features + threads.x-1)/threads.x,(elem+threads.y-1)/threads.y,1};\n",
        "            setBatch<<<blocks, threads>>>(dev_x_train, dev_x_input, elem, n_features, m);\n",
        "\n",
        "            // Feedforward\n",
        "            MLP_MSELIN_forward(elem);\n",
        "\n",
        "\n",
        "            // output for this minibatch\n",
        "            threads ={1,tiley,1};\n",
        "            blocks ={(n_output + threads.x-1)/threads.x,(elem+threads.y-1)/threads.y,1};\n",
        "            setBatch<<<blocks, threads>>>(dev_y_train, dev_y_input, elem, n_output, m);\n",
        "\n",
        "            float res = MLP_MSE_cost(elem);\n",
        "            float step_cost = res / (2.0f * elem);\n",
        "            printf(\"Epoch %d/%d, minibatch %04d, Loss (MSE) %g\\n\", e, epochs, m, step_cost);\n",
        "\n",
        "            // Compute gradient via backpropagation\n",
        "            MLP_MSELIN_backprop();\n",
        "\n",
        "            threads ={tilex,tiley,1};\n",
        "            blocks ={((n_features+1) + threads.x-1)/threads.x,(n_hidden + threads.y-1)/threads.y,1};\n",
        "            update_weights<<<blocks, threads>>>(dev_delta_W1_unscaled, dev_w1, n_hidden , (n_features+1));\n",
        "\n",
        "            blocks ={((n_hidden+1) + threads.x-1)/threads.x,(n_output+ threads.y-1)/threads.y,1};\n",
        "            update_weights<<<blocks, threads>>>(dev_delta_W2_unscaled, dev_w2, n_output, (n_hidden+1));\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(&w1[0][0], dev_w1, n_hidden * (n_features+1) * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&w2[0][0], dev_w2, n_output * (n_hidden+1) * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKRedPCkHSEd",
        "outputId": "b0d59ea0-ee6f-491c-9170-6c7a5534c91c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict"
      ],
      "metadata": {
        "id": "aUaI6t77HYKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.cu\n",
        "\n",
        "// Predict the outputs for all the observations in X, where each row of X is a distinct observation.\n",
        "float MLP_MSELIN_predict(float* x, float* y, int tot_elem) {\n",
        "    float sum = 0;\n",
        "\n",
        "    cudaMalloc((void **)&dev_x_pred, tot_elem * n_features * sizeof(float));\n",
        "    cudaMemcpy(dev_x_pred, x, tot_elem * n_features * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMalloc((void **)&dev_y_pred,  tot_elem * n_output * sizeof(float));\n",
        "    cudaMemcpy(dev_y_pred, y, tot_elem * n_output * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    for (int m = 1; m * elem <= tot_elem+(elem-1); ++m) {   // precautions in case tot_elem is not a multiple of elem\n",
        "        int current_elem = (m * elem <= tot_elem) ? elem : (tot_elem - (m - 1) * elem);\n",
        "\n",
        "        dim3 threads ={tilex,tiley,1};\n",
        "        dim3 blocks ={(n_features + threads.x-1)/threads.x,(current_elem+threads.y-1)/threads.y,1};\n",
        "        setBatch<<<blocks, threads>>>(dev_x_pred, dev_x_input, current_elem, n_features, m);\n",
        "\n",
        "        blocks ={(n_output + threads.x-1)/threads.x,(current_elem+threads.y-1)/threads.y,1};\n",
        "        setBatch<<<blocks, threads>>>(dev_y_pred, dev_y_input, current_elem, n_output, m);\n",
        "\n",
        "        MLP_MSELIN_forward(current_elem);\n",
        "\n",
        "        sum += MLP_MSE_cost(current_elem);\n",
        "    }\n",
        "\n",
        "    float step_cost = sum / (2.0f * tot_elem);\n",
        "\n",
        "    cudaFree(dev_x_pred);\n",
        "    cudaFree(dev_y_pred);\n",
        "\n",
        "    return step_cost;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvrvWvrZHh-m",
        "outputId": "8c66c451-d320-422d-aa97-5705a0fee839"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main"
      ],
      "metadata": {
        "id": "2pjvQOMSHk6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"MLP.cuh\"\n",
        "#include \"gen.cu\"\n",
        "#include \"train.cu\"\n",
        "#include \"predict.cu\"\n",
        "\n",
        "int main() {\n",
        "    array<array<float, n_features>, num_train> x_train;\n",
        "    array<float, num_train> y_train;\n",
        "    sinc2D_gen(x_train[0].data(), y_train.data(), num_train);\n",
        "\n",
        "    array<array<float, n_features>, num_test> x_test;\n",
        "    array<float, num_test> y_test;\n",
        "    sinc2D_gen(x_test[0].data(), y_test.data(), num_test);\n",
        "\n",
        "\n",
        "\n",
        "    // Shuffling training data\n",
        "    array<int, num_train> shuffled_ind;\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        shuffled_ind[i] = i;\n",
        "    }\n",
        "\n",
        "    default_random_engine generator(std::time(nullptr));\n",
        "    shuffle(shuffled_ind.begin(), shuffled_ind.end(), generator);\n",
        "\n",
        "    array<array<float, n_features>, num_train> x_train_temp;\n",
        "    array<float, num_train> y_train_temp;\n",
        "\n",
        "    for (int i = 0; i < num_train; ++i) {\n",
        "        x_train_temp[i] = x_train[shuffled_ind[i]];\n",
        "        y_train_temp[i] = y_train[shuffled_ind[i]];\n",
        "    }\n",
        "\n",
        "    x_train = x_train_temp;\n",
        "    y_train = y_train_temp;\n",
        "\n",
        "    // Learn weights from training data\n",
        "    MLP_MSELIN_train(x_train, y_train);\n",
        "\n",
        "    // Predict the outputs for all the observations in X\n",
        "    float acc_train = MLP_MSELIN_predict(x_train[0].data(), y_train.data(), num_train);\n",
        "    printf(\"Training accuracy (MSE): %g\\n\", acc_train);\n",
        "\n",
        "    float acc_test = MLP_MSELIN_predict(x_test[0].data(), y_test.data(), num_test);\n",
        "    printf(\"Test accuracy: (MSE): %g\\n\", acc_test);\n",
        "\n",
        "    GPU_free();\n",
        "\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6anHHCa5Hpbz",
        "outputId": "8bcf3159-9e1a-4d0e-8fc2-65bb0eddbf89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run"
      ],
      "metadata": {
        "id": "DnCIhYU0Hsdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm prog\n",
        "!nvcc -o prog main.cu"
      ],
      "metadata": {
        "id": "MuBzO9-UHvPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./prog"
      ],
      "metadata": {
        "id": "VWk-72uiHxcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}